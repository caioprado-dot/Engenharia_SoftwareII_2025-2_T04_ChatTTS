path,type,chunk_index,text,file_category
README.md,documentation,0,"<div align=""center"">

<a href=""https://trendshift.io/repositories/10489"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/10489"" alt=""2noise%2FChatTTS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

# ChatTTS
A generative speech model for daily dialogue.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)",documentation
README.md,documentation,1,"[![Huggingface](https://img.shields.io/badge/ %20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

**English** | [** **](docs/cn/README.md) | [** **](docs/jp/README.md) | [** **](docs/ru/README.md) | [**Espa ol**](docs/es/README.md) | [**Fran ais**](docs/fr/README.md) | [** **](docs/kr/README.md)

</div>

## Introduction
> [!Note]
> This repo contains the algorithm infrastructure and some simple examples.

> [!Tip]
> For the extended end-user products, please refer to the index repo [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS/tree/en) maintained by the community.",documentation
README.md,documentation,2,"ChatTTS is a text-to-speech model designed specifically for dialogue scenarios such as LLM assistant.

### Supported Languages
- [x] English
- [x] Chinese
- [ ] Coming Soon...

### Highlights
> You can refer to **[this video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** for the detailed description.

1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.
2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. 
3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.

### Dataset & Model
> [!Important]
> The released model is for academic purposes only.",documentation
README.md,documentation,3,"- The main model is trained with Chinese and English audio data of 100,000+ hours.
- The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre-trained model without SFT.

### Roadmap
- [x] Open-source the 40k-hours-base model and spk_stats file.
- [x] Streaming audio generation.
- [x] Open-source DVAE encoder and zero shot inferring code.
- [ ] Multi-emotion controlling.
- [ ] ChatTTS.cpp (new repo in `2noise` org is welcomed)

### Licenses

#### The Code

The code is published under `AGPLv3+` license.

#### The model",documentation
README.md,documentation,4,"The model is published under `CC BY-NC 4.0` license. It is intended for educational and research use, and should not be used for any commercial or illegal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.

### Disclaimer

ChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.",documentation
README.md,documentation,5,"### Contact
> GitHub issues/PRs are always welcomed.

#### Formal Inquiries
For formal inquiries about the model and roadmap, please contact us at **open-source@2noise.com**.

#### Online Chat
##### 1. QQ Group (Chinese Social APP)
- **Group 1**, 808364215
- **Group 2**, 230696694
- **Group 3**, 933639842
- **Group 4**, 608667975

##### 2. Discord Server
Join by clicking [here](https://discord.gg/Ud5Jxgx5yD).

## Get Started
### Clone Repo
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Install requirements
#### 1. Install Directly
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Install from conda
```bash
conda create -n chattts python=3.11
conda activate chattts
pip install -r requirements.txt
```

#### Optional: Install vLLM (Linux only)
```bash
pip install safetensors vllm==0.2.7 torchaudio
```",documentation
README.md,documentation,6,"#### Unrecommended Optional: Install TransformerEngine if using NVIDIA GPU (Linux only)
> [!Warning]
> DO NOT INSTALL! 
> The adaptation of TransformerEngine is currently under development and CANNOT run properly now. 
> Only install it on developing purpose. See more details on at #672 #676

> [!Note]
> The installation process is very slow.

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### Unrecommended Optional: Install FlashAttention-2 (mainly NVIDIA GPU)
> [!Warning]
> DO NOT INSTALL! 
> Currently the FlashAttention-2 will slow down the generating speed according to [this issue](https://github.com/huggingface/transformers/issues/26990). 
> Only install it on developing purpose.

> [!Note]
> See supported devices at the [Hugging Face Doc](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).

```bash
pip install flash-attn --no-build-isolation
```",documentation
README.md,documentation,7,"### Quick Start
> Make sure you are under the project root directory when you execute these commands below.

#### 1. Launch WebUI
```bash
python examples/web/webui.py
```

#### 2. Infer by Command Line
> It will save audio to `./output_audio_n.mp3`

```bash
python examples/cmd/run.py ""Your text 1."" ""Your text 2.""
```

## Installation

1. Install the stable version from PyPI
```bash
pip install ChatTTS
```

2. Install the latest version from GitHub
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. Install from local directory in dev mode
```bash
pip install -e .
```

### Basic Usage

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = [""PUT YOUR 1st TEXT HERE"", ""PUT YOUR 2nd TEXT HERE""]

wavs = chat.infer(texts)",documentation
README.md,documentation,8,"for i in range(len(wavs)):
    """"""
    In some versions of torchaudio, the first line works but in other versions, so does the second line.
    """"""
    try:
        torchaudio.save(f""basic_output{i}.wav"", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)
    except:
        torchaudio.save(f""basic_output{i}.wav"", torch.from_numpy(wavs[i]), 24000)
```

### Advanced Usage

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.",documentation
README.md,documentation,9,"# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.

text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
""""""
In some versions of torchaudio, the first line works but in other versions, so does the second line.
""""""
try:
    torchaudio.save(""word_level_output.wav"", torch.from_numpy(wavs[0]).unsqueeze(0), 24000)
except:
    torchaudio.save(""word_level_output.wav"", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Example: self introduction</h4></summary>",documentation
README.md,documentation,10,"```python
inputs_en = """"""
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements like 
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
"""""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save(""self_introduction_output.wav"", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align=""center"">

**male speaker**

</td>
<td align=""center"">

**female speaker**

</td>
</tr>
<tr>
<td align=""center"">",documentation
README.md,documentation,11,"[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align=""center"">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

## FAQ

#### 1. How much VRAM do I need? How about infer speed?
For a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.

#### 2. Model stability is not good enough, with issues such as multi speakers or poor audio quality.

This is a problem that typically occurs with autoregressive models (for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.

#### 3. Besides laughter, can we control anything else? Can we control other emotions?",documentation
README.md,documentation,12,"In the current released model, the only token-level control units are `[laugh]`, `[uv_break]`, and `[lbreak]`. In future versions, we may open-source models with additional emotional control capabilities.

## Acknowledgements
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demonstrate a remarkable TTS result by an autoregressive-style system.
- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.
- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.

## Special Appreciation
- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.

## Thanks to all contributors for their efforts
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align=""center"">",documentation
README.md,documentation,13,"  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>",documentation
examples/onnx/README.md,documentation,0,"# Export onnx or JIT models for deployment

## Run `pip install onnx -U`.

## Export GPT

3. Run `python examples/onnx/exporter.py --gpt`

## Export other models
Run `python examples/onnx/exporter.py --decoder --vocos`

## Reference
[Run LLMs on Sophon TPU](https://github.com/sophgo/LLM-TPU)",documentation
examples/api/README.md,documentation,0,"# Generating voice with ChatTTS via API

## Install requirements

Install `FastAPI` and `requests`:

```
pip install -r examples/api/requirements.txt
```

## Run API server

```
fastapi dev examples/api/main.py --host 0.0.0.0 --port 8000
```

## Run openAI_API server

```
fastapi dev examples/api/openai_api.py --host 0.0.0.0 --port 8000
```
## Generate audio using requests

```
python examples/api/client.py
```

mp3 audio files will be saved to the `output` directory.",documentation
docs/ru/README.md,documentation,0,"# ChatTTS
> [!NOTE]
>              ,  ,            .

[![Huggingface](https://img.shields.io/badge/ %20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)

[**English**](../../README.md) | [** **](../cn/README.md) | [** **](../jp/README.md) | ** ** | [**Espa ol**](../es/README.md) | [**Fran ais**](../fr/README.md) | [** **](../kr/README.md)

ChatTTS -            ,          ,       LLM.        ,        .             100 000          .       **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** -           40 000     SFT.

               ,  ,           **open-source@2noise.com**.             QQ: 808364215    .       GitHub    .

---
##  
1. **  TTS**: ChatTTS      ,      ,              .        ,      .
2. **   **:                ,    ,        .
3. **   **: ChatTTS         TTS        .                      .

                **[    Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**

---

##      ",documentation
docs/ru/README.md,documentation,1,"             .                                .        ,        .      ,        ,              .          ,            -               .

ChatTTS -            .                  .       ChatTTS,                       40 000                       MP3,                .                              .

---
##  

<h4>   </h4>

```python
import ChatTTS
from IPython.display import Audio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) #     True      

texts = [""       "",]

wavs = chat.infer(texts)

torchaudio.save(""output1.wav"", torch.from_numpy(wavs[0]), 24000)
```

<h4>   </h4>

```python
###################################
#        .

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = {
  'spk_emb': rand_spk, #      
  'temperature': .3, #      
  'top_P': 0.7, #   top P
  'top_K': 20, #   top K
}

###################################
#          .",documentation
docs/ru/README.md,documentation,2,"#   oral_(0-9), laugh_(0-2), break_(0-7)
#                .
params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_6]'
} 

wav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)

###################################
#          .
text = '         ?[uv_break]your favorite english food?[laugh][lbreak]'
wav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save(""output2.wav"", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4> :  </h4></summary>

```python
inputs_ru = """"""
ChatTTS -            ,        . 
[uv_break]          [uv_break]          
            [laugh]  [uv_break] [laugh], [uv_break] , [uv_break]   . 
[uv_break]           ,[uv_break] ,  ,
[uv_break]                  .[uv_break]
"""""".replace('\n', '') #                .",documentation
docs/ru/README.md,documentation,3,"params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_4]'
} 
audio_array_ru = chat.infer(inputs_ru, params_refine_text=params_refine_text)
torchaudio.save(""output3.wav"", torch.from_numpy(audio_array_ru[0]), 24000)
```
[   ](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

[   ](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)
</details>

---
##    
- [x]             40         spk_stats
- [ ]         VQ       Lora
- [ ]            *
- [ ]           40            
- [ ] ChatTTS.cpp  ? (PR        .)

----
##      

#####   VRAM    ?        ?
  30-          4     GPU.   GPU 4090,        ,     7        .       (RTF)     0.3.

#####          ,                  .

   ,             (  bark   valle).        .        ,        .

#####    ,          -   ?            ?

                      [laugh], [uv_break]   [lbreak].                        .",documentation
docs/ru/README.md,documentation,4,"---
##  
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS)   [valle](https://arxiv.org/abs/2301.02111)       TTS          .
- [fish-speech](https://github.com/fishaudio/fish-speech)     GVQ           LLM.
- [vocos](https://github.com/gemelo-ai/vocos),              .

---
##    
- [wlu-audio lab](https://audio.westlake.edu.cn/)          .",documentation
docs/cn/README.md,documentation,0,"<div align=""center"">

<a href=""https://trendshift.io/repositories/10489"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/10489"" alt=""2noise%2FChatTTS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

# ChatTTS
 

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)",documentation
docs/cn/README.md,documentation,1,"[![Huggingface](https://img.shields.io/badge/ %20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | ** ** | [** **](../jp/README.md) | [** **](../ru/README.md) | [**Espa ol**](../es/README.md) | [**Fran ais**](../fr/README.md) | [** **](../kr/README.md)

</div>

> [!NOTE]
>  

##  

> [!Note]
>  

> [!Tip]
>    [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS) 

ChatTTS   LLM  

###  

- [x]  
- [x]  
- [ ]  ...

###  

>   **[Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**  

1. **  TTS**: ChatTTS  
2. ** **:  
3. ** **: ChatTTS   TTS  

###  ",documentation
docs/cn/README.md,documentation,2,"-   100,000+  
- **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)**   40,000  

###  

- [x]   4   spk_stats  
- [x]  
- [x]   DVAE  
- [ ]   4  
- [ ] ChatTTS.cpp (  2noise  ) 

###  

> [!Important]
>  

 

ChatTTS   ChatTTS   40,000   MP3  

###  

>   GitHub issues/PRs 

####  

  **open-source@2noise.com** 

####  

##### 1.   QQ  

- **  1**, 808364215 ( )
- **  2**, 230696694 ( )
- **  3**, 933639842 ( )
- **  4**, 608667975

##### 2. Discord

  [Discord](https://discord.gg/Ud5Jxgx5yD) 

##  

###  

```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

###  

#### 1.  

```bash
pip install --upgrade -r requirements.txt
```

#### 2.   conda  

```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

####   :   NVIDIA GPU  Linux  TransformerEngine 

> [!Note]
>  

> [!Warning]
> TransformerEngine  

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

####   :   FlashAttention-2 (  NVIDIA GPU)",documentation
docs/cn/README.md,documentation,3,"> [!Note]
>   [Hugging Face Doc](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).

```bash
pip install flash-attn --no-build-isolation
```

###  

>  

#### 1. WebUI  

```bash
python examples/web/webui.py
```

#### 2.  

>   `./output_audio_n.mp3`

```bash
python examples/cmd/run.py ""Your text 1."" ""Your text 2.""
```

##  

###   Python  

1.   PyPI  

```bash
pip install ChatTTS
```

2.   GitHub  

```bash
pip install git+https://github.com/2noise/ChatTTS
```

3.  

```bash
pip install -e .
```

###  

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = [""PUT YOUR 1st TEXT HERE"", ""PUT YOUR 2nd TEXT HERE""]

wavs = chat.infer(texts)

torchaudio.save(""output1.wav"", torch.from_numpy(wavs[0]), 24000)
```

###  

```python
###################################
# Sample a speaker from Gaussian.",documentation
docs/cn/README.md,documentation,4,"rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.",documentation
docs/cn/README.md,documentation,5,"text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save(""output2.wav"", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4> :  </h4></summary>

```python
inputs_en = """"""
chatTTS is a text to speech model designed for dialogue applications.
[uv_break]it supports mixed language input [uv_break]and offers multi speaker
capabilities with precise control over prosodic elements like
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation.
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
"""""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)",documentation
docs/cn/README.md,documentation,6,"audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save(""output3.wav"", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align=""center"">

** **

</td>
<td align=""center"">

** **

</td>
</tr>
<tr>
<td align=""center"">

[ ](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align=""center"">

[ ](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

##  

#### 1.   VRAM   

  30   4GB   GPU     4090 GPU  7   token   (RTF)   0.3 

#### 2.  

  bark   valle 

#### 3.  

  token   `[laugh]`, `[uv_break]`   `[lbreak]` 

##  

- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS)   [valle](https://arxiv.org/abs/2301.02111)   TTS  
- [fish-speech](https://github.com/fishaudio/fish-speech)   GVQ   LLM  
- [vocos](https://github.com/gemelo-ai/vocos) vocos  

##  ",documentation
docs/cn/README.md,documentation,7,"- [wlu-audio lab](https://audio.westlake.edu.cn/)  

##  

[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

##  

<div align=""center"">

![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>",documentation
docs/jp/README.md,documentation,0,"# ChatTTS
> [!NOTE]
>  

[![Huggingface](https://img.shields.io/badge/ %20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)

[**English**](../../README.md) | [** **](../cn/README.md) | ** ** | [** **](../ru/README.md) | [**Espa ol**](../es/README.md) | [**Fran ais**](../fr/README.md) | [** **](../kr/README.md)

ChatTTS LLM 100,000 **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** 40,000 SFT 

 **open-source@2noise.com** QQ 808364215 GitHub 

---
##  
1. ** TTS**: ChatTTS 
2. ** **:  
3. ** **: ChatTTS TTS 

 **[Bilibili ](https://www.bilibili.com/video/BV1zn4y1o7iV)** 

---

##  

 

ChatTTS ChatTTS 40,000 MP3 

---
##  

<h4> </h4>

```python
import ChatTTS
from IPython.display import Audio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) #  True 

texts = ["" "",]

wavs = chat.infer(texts, )

torchaudio.save(""output1.wav"", torch.from_numpy(wavs[0]), 24000)
```

<h4> </h4>

```python
###################################
#  ",documentation
docs/jp/README.md,documentation,1,"rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = {
  'spk_emb': rand_spk, #  
  'temperature': .3, #  
  'top_P': 0.7, #  P 
  'top_K': 20, #  K 
}

###################################
#  

#  oral_(0-9) laugh_(0-2) break_(0-7) 
params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_6]'
} 

wav = chat.infer(texts, params_refine_text=params_refine_text, params_infer_code=params_infer_code)

###################################
#  
text = ' [uv_break][laugh][lbreak]'
wav = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save(""output2.wav"", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4> </h4></summary>

```python
inputs_jp = """"""
ChatTTS 
[uv_break] [uv_break] [laugh] 
[uv_break] [laugh] [uv_break] [uv_break] [uv_break] 
[uv_break] [uv_break]
"""""".replace('\n', '') #  ",documentation
docs/jp/README.md,documentation,2,"params_refine_text = {
  'prompt': '[oral_2][laugh_0][break_4]'
} 
audio_array_jp = chat.infer(inputs_jp, params_refine_text=params_refine_text)
torchaudio.save(""output3.wav"", torch.from_numpy(audio_array_jp[0]), 24000)
```
[ ](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

[ ](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)
</details>

---
##  
- [x] 40k spk_stats 
- [ ] VQ Lora 
- [ ]  *
- [ ]  40k 
- [ ] ChatTTS.cpp PR 

----
## FAQ

##### VRAM 
30 4GB GPU 4090 GPU 7 1 RTF 0.3 

#####  

 bark valle 

#####  

 [laugh] [uv_break] [lbreak] 

---
##  
- [bark](https://github.com/suno-ai/bark) [XTTSv2](https://github.com/coqui-ai/TTS) [valle](https://arxiv.org/abs/2301.02111) TTS 
- [fish-speech](https://github.com/fishaudio/fish-speech) LLM GVQ 
-  [vocos](https://github.com/gemelo-ai/vocos) 

---
##  
-  [wlu-audio lab](https://audio.westlake.edu.cn/)",documentation
docs/es/README.md,documentation,0,"<div align=""center"">

<a href=""https://trendshift.io/repositories/10489"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/10489"" alt=""2noise%2FChatTTS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

# ChatTTS
Un modelo de generaci n de voz para la conversaci n diaria.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)

[![Huggingface](https://img.shields.io/badge/ %20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)

[**English**](../../README.md) | [** **](../cn/README.md) | [** **](../jp/README.md) | [** **](../ru/README.md) | **Espa ol**
 | [**Fran ais**](../fr/README.md) | [** **](../kr/README.md)
</div>",documentation
docs/es/README.md,documentation,1,"> [!NOTE]
> Atenci n, es posible que esta versi n no sea la  ltima. Por favor, consulte la versi n en ingl s para conocer todo el contenido.

## Introducci n

ChatTTS es un modelo de texto a voz dise ado espec ficamente para escenarios conversacionales como LLM assistant.

### Idiomas Soportados

- [x] Ingl s
- [x] Chino
- [ ] Mant nganse al tanto...

### Aspectos Destacados

> Puede consultar **[este video en Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** para obtener una descripci n detallada.",documentation
docs/es/README.md,documentation,2,"1. **TTS Conversacional**: ChatTTS est  optimizado para tareas conversacionales, logrando una s ntesis de voz natural y expresiva. Soporta m ltiples hablantes, lo que facilita la generaci n de di logos interactivos.
2. **Control Finas**: Este modelo puede predecir y controlar caracter sticas detalladas de la prosodia, incluyendo risas, pausas e interjecciones.
3. **Mejor Prosodia**: ChatTTS supera a la mayor a de los modelos TTS de c digo abierto en cuanto a prosodia. Ofrecemos modelos preentrenados para apoyar estudios y desarrollos adicionales.

### Conjunto de Datos & Modelo

- El modelo principal se entrena con m s de 100.000 horas de datos de audio en chino e ingl s.
- La versi n de c digo abierto en **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** es un modelo preentrenado con 40.000 horas, sin SFT.

### Hoja de Ruta",documentation
docs/es/README.md,documentation,3,"- [x] Publicar el modelo base de 40k horas y el archivo spk_stats como c digo abierto
- [ ] Publicar los c digos de codificador VQ y entrenamiento de Lora como c digo abierto
- [ ] Generaci n de audio en streaming sin refinar el texto
- [ ] Publicar la versi n de 40k horas con control de m ltiples emociones como c digo abierto
- [ ]  ChatTTS.cpp? (Se aceptan PR o un nuevo repositorio)

### Descargo de Responsabilidad

> [!Important]
> Este repositorio es s lo para fines acad micos.

Este proyecto est  destinado a fines educativos y estudios, y no es adecuado para ning n prop sito comercial o legal. El autor no garantiza la exactitud, integridad o fiabilidad de la informaci n. La informaci n y los datos utilizados en este repositorio son  nicamente para fines acad micos y de investigaci n. Los datos provienen de fuentes p blicas, y el autor no reclama ning n derecho de propiedad o copyright sobre ellos.",documentation
docs/es/README.md,documentation,4,"ChatTTS es un potente sistema de conversi n de texto a voz. Sin embargo, es crucial utilizar esta tecnolog a de manera responsable y  tica. Para limitar el uso de ChatTTS, hemos a adido una peque a cantidad de ruido de alta frecuencia durante el proceso de entrenamiento del modelo de 40.000 horas y hemos comprimido la calidad del audio en formato MP3 tanto como sea posible para evitar que actores malintencionados lo usen con fines delictivos. Adem s, hemos entrenado internamente un modelo de detecci n y planeamos hacerlo de c digo abierto en el futuro.

### Contacto

> No dudes en enviar issues/PRs de GitHub.

#### Consultas Formales

Si desea discutir la cooperaci n sobre modelos y hojas de ruta, env e un correo electr nico a **open-source@2noise.com**.

#### Chat en L nea

##### 1. Grupo QQ (Aplicaci n Social China)

- **Grupo 1**, 808364215 (Lleno)
- **Grupo 2**, 230696694 (Lleno)
- **Grupo 3**, 933639842

## Instalaci n (En Proceso)",documentation
docs/es/README.md,documentation,5,"> Se cargar  en pypi pronto seg n https://github.com/2noise/ChatTTS/issues/269.

```bash
pip install git+https://github.com/2noise/ChatTTS
```

## Inicio
### Clonar el repositorio
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Requerimientos de instalaci n
#### 1. Instalar directamente
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Instalar desde conda
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

### Inicio R pido
#### 1. Iniciar la interfaz de usuario web (WebUI)
```bash
python examples/web/webui.py
```

#### 2. Inferir por l nea de comando
> Guardar  el audio en `./output_audio_xxx.wav`

```bash
python examples/cmd/run.py ""Please input your text.""
```

### B sico

```python
import ChatTTS
from IPython.display import Audio
import torchaudio
import torch

chat = ChatTTS.Chat()
chat.load(compile=False) # Set to True for better performance

texts = [""PUT YOUR TEXT HERE"",]

wavs = chat.infer(texts)",documentation
docs/es/README.md,documentation,6,"torchaudio.save(""output1.wav"", torch.from_numpy(wavs[0]), 24000)
```

### Avanzado

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)",documentation
docs/es/README.md,documentation,7,"###################################
# For word level manual control.
text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save(""output2.wav"", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Ejemplo: auto presentaci n</h4></summary>

```python
inputs_en = """"""
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements [laugh]like like 
[uv_break]laughter[laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
"""""".replace('\n', '') # English is still experimental.",documentation
docs/es/README.md,documentation,8,"params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save(""output3.wav"", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align=""center"">

**altavoz masculino**

</td>
<td align=""center"">

**altavoz femenino**

</td>
</tr>
<tr>
<td align=""center"">

[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align=""center"">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

## Preguntas y Respuestas",documentation
docs/es/README.md,documentation,9,"#### 1.  Cu nta memoria gr fica de acceso aleatorio necesito?  Qu  tal inferir la velocidad?
Para un clip de audio de 30 segundos, se requieren al menos 4 GB de memoria de GPU. Para la GPU 4090, puede generar audio correspondiente a aproximadamente 7 tokens sem nticos por segundo. El Factor en Tiempo Real (RTF) es aproximadamente 0,3.

#### 2. La estabilidad del modelo no es lo suficientemente buena y existen problemas como varios altavoces o mala calidad del sonido.

Este es un problema com n en los modelos autorregresivos (para bark y valle). Generalmente es dif cil de evitar. Puede probar varias muestras para encontrar resultados adecuados.

#### 3.  Podemos controlar algo m s que la risa?  Podemos controlar otras emociones?

En el modelo lanzado actualmente, las  nicas unidades de control a nivel de token son `[risa]`, `[uv_break]` y `[lbreak]`. En una versi n futura, es posible que abramos el c digo fuente del modelo con capacidades adicionales de control de emociones.",documentation
docs/es/README.md,documentation,10,"## Agradecimientos
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) y [valle](https://arxiv.org/abs/2301.02111) demuestran un resultado TTS notable mediante un sistema de estilo autorregresivo.
- [fish-speech](https://github.com/fishaudio/fish-speech) revela las capacidades de GVQ como tokenizador de audio para el modelado LLM.
- [vocos](https://github.com/gemelo-ai/vocos) se utiliza como codificador de voz previamente entrenado.

## Agradecimiento Especial
- [wlu-audio lab](https://audio.westlake.edu.cn/) para experimentos iniciales del algoritmo.

## Recursos Relacionados
- [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS)

## Gracias a todos los contribuyentes por sus esfuerzos.
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align=""center"">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>",documentation
docs/kr/README.md,documentation,0,"<div align=""center"">

<a href=""https://trendshift.io/repositories/10489"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/10489"" alt=""2noise%2FChatTTS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

# ChatTTS
           .

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)",documentation
docs/kr/README.md,documentation,1,"[![Huggingface](https://img.shields.io/badge/ %20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | [** **](../cn/README.md) | [** **](../jp/README.md) | [** **](../ru/README.md) | [**Espa ol**](../es/README.md) | [**Fran ais**](../fr/README.md) | ** **

</div>

> [!NOTE]
>              . [   ](../../README.md)         .

##    

> [!Note]
>                .

> [!Tip]
>                [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS)     .

ChatTTS       ( : LLM  )       -   (TTS)  .

###    

- [x]  
- [x]  
- [ ]      ...

###    ",documentation
docs/kr/README.md,documentation,2,">       **[Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)**         .

1. **  TTS**: ChatTTS                     .              .
2. **   **:      ,    ,                  .
3. **   **: ChatTTS            TTS    ,                  .

###      
> [!Important]
>            .

-     100,000+                .
- **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)**          40,000       , SFT     .

###  
- [x] 40,000      spk_stats      .
- [x]      .
- [x] DVAE              .
- [ ]        .
- [ ] ChatTTS.cpp (`2noise`          .)

###  

####  
  `AGPLv3+`    .

####  
  `CC BY-NC 4.0`    .              ,              .      ,  ,      .                    ,            .                .

###    

ChatTTS     -     .                  . ChatTTS        40,000               ,         MP3    .  ,          ,          .

###  
> GitHub  /PR     .

####    
            **open-source@2noise.com**     .",documentation
docs/kr/README.md,documentation,3,"####    
##### 1. QQ Group (Chinese Social APP)
- **Group 1**, 808364215
- **Group 2**, 230696694
- **Group 3**, 933639842
- **Group 4**, 608667975

##### 2. Discord  
[ ](https://discord.gg/Ud5Jxgx5yD)     .

##  
###    
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

###    
#### 1.    
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Conda   
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

####  : vLLM   (Linux  )
```bash
pip install safetensors vllm==0.2.7 torchaudio
```

####      : NVIDIA GPU     TransformerEngine   (Linux  )
> [!Warning]
>    !
> TransformerEngine           ,        .
>      .     #672   #676       .

> [!Note]
>        .

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

####      : FlashAttention-2   (  NVIDIA GPU)
> [!Warning]
>    !
>   FlashAttention-2  [   ](https://github.com/huggingface/transformers/issues/26990)         .
>      .",documentation
docs/kr/README.md,documentation,4,"> [!Note]
>     [Hugging Face  ](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)       .

```bash
pip install flash-attn --no-build-isolation
```

###    
>                  .

#### 1. WebUI  
```bash
python examples/web/webui.py
```

#### 2.      
>   `./output_audio_n.mp3`   .

```bash
python examples/cmd/run.py ""Your text 1."" ""Your text 2.""
```

##    

1. PyPI       
```bash
pip install ChatTTS
```

2. GitHub       
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3.          
```bash
pip install -e .
```

###    

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) #       True     

texts = [""PUT YOUR 1st TEXT HERE"", ""PUT YOUR 2nd TEXT HERE""]

wavs = chat.infer(texts)",documentation
docs/kr/README.md,documentation,5,"for i in range(len(wavs)):
    """"""
    torchaudio                 ,                .
    """"""
    try:
        torchaudio.save(f""basic_output{i}.wav"", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)
    except:
        torchaudio.save(f""basic_output{i}.wav"", torch.from_numpy(wavs[i]), 24000)
```

### Advanced Usage

```python
###################################
# Sample a speaker from Gaussian.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # save it for later timbre recovery

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # add sampled speaker 
    temperature = .3,   # using custom temperature
    top_P = 0.7,        # top P decode
    top_K = 20,         # top K decode
)

###################################
# For sentence level manual control.

# use oral_(0-9), laugh_(0-2), break_(0-7) 
# to generate special token in text to synthesize.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)",documentation
docs/kr/README.md,documentation,6,"wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# For word level manual control.

text = 'What is [uv_break]your favorite english food?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
""""""
In some versions of torchaudio, the first line works but in other versions, so does the second line.
""""""
try:
    torchaudio.save(""word_level_output.wav"", torch.from_numpy(wavs[0]).unsqueeze(0), 24000)
except:
    torchaudio.save(""word_level_output.wav"", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Example: self introduction</h4></summary>",documentation
docs/kr/README.md,documentation,7,"```python
inputs_en = """"""
chat T T S is a text to speech model designed for dialogue applications. 
[uv_break]it supports mixed language input [uv_break]and offers multi speaker 
capabilities with precise control over prosodic elements like 
[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. 
[uv_break]it delivers natural and expressive speech,[uv_break]so please
[uv_break] use the project responsibly at your own risk.[uv_break]
"""""".replace('\n', '') # English is still experimental.

params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save(""self_introduction_output.wav"", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align=""center"">

**male speaker**

</td>
<td align=""center"">

**female speaker**

</td>
</tr>
<tr>
<td align=""center"">",documentation
docs/kr/README.md,documentation,8,"[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align=""center"">

[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

## FAQ

#### 1. VRAM     ?        ?
30            4GB  GPU    . 4090 GPU        7               .    (RTF)    0.3 .

#### 2.      ,                  .

    autoregressive  (bark   valle  )       .                  .

#### 3.                ?

            `[laugh]`, `[uv_break]`, `[lbreak]` .                        .

##    
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS), [valle](https://arxiv.org/abs/2301.02111)  autoregressive       TTS    .
- [fish-speech](https://github.com/fishaudio/fish-speech)  LLM         GVQ     .
- [vocos](https://github.com/gemelo-ai/vocos)      vocoder   .

##    
-         [wlu-audio lab](https://audio.westlake.edu.cn/)       .",documentation
docs/kr/README.md,documentation,9,"##        
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align=""center"">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>",documentation
docs/fr/README.md,documentation,0,"<div align=""center"">

<a href=""https://trendshift.io/repositories/10489"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/10489"" alt=""2noise%2FChatTTS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

# ChatTTS
Un mod le de parole g n ratif pour le dialogue quotidien.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)",documentation
docs/fr/README.md,documentation,1,"[![Huggingface](https://img.shields.io/badge/ %20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)

[**English**](../../README.md) | [** **](../cn/README.md) | [** **](../jp/README.md) | [** **](../ru/README.md) | [**Espa ol**](../es/README.md)| **Fran ais**  | [** **](../kr/README.md)

</div>

## Introduction
> [!Note]
> Ce d p t contient l'infrastructure de l'algorithme et quelques exemples simples.

> [!Tip]
> Pour les produits finaux  tendus pour les utilisateurs, veuillez consulter le d p t index [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS/tree/en) maintenu par la communaut .",documentation
docs/fr/README.md,documentation,2,"ChatTTS est un mod le de synth se vocale con u sp cifiquement pour les sc narios de dialogue tels que les assistants LLM.

### Langues prises en charge
- [x] Anglais
- [x] Chinois
- [ ]   venir...

### Points forts
> Vous pouvez vous r f rer   **[cette vid o sur Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** pour une description d taill e.

1. **Synth se vocale conversationnelle**: ChatTTS est optimis  pour les t ches bas es sur le dialogue, permettant une synth se vocale naturelle et expressive. Il prend en charge plusieurs locuteurs, facilitant les conversations interactives.
2. **Contr le granulaire**: Le mod le peut pr dire et contr ler des caract ristiques prosodiques fines, y compris le rire, les pauses et les interjections.
3. **Meilleure prosodie**: ChatTTS d passe la plupart des mod les TTS open-source en termes de prosodie. Nous fournissons des mod les pr -entra n s pour soutenir la recherche et le d veloppement.",documentation
docs/fr/README.md,documentation,3,"### Dataset & Mod le
- Le mod le principal est entra n  avec des donn es audio en chinois et en anglais de plus de 100 000 heures.
- La version open-source sur **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** est un mod le pr -entra n  de 40 000 heures sans SFT.

### Roadmap
- [x] Open-source du mod le de base de 40k heures et du fichier spk_stats.
- [x] G n ration audio en streaming.
- [ ] Open-source de la version 40k heures avec contr le multi- motions.
- [ ] ChatTTS.cpp (nouveau d p t dans l'organisation `2noise` est bienvenu)

### Avertissement
> [!Important]
> Ce d p t est   des fins acad miques uniquement.",documentation
docs/fr/README.md,documentation,4,"Il est destin    un usage  ducatif et de recherche, et ne doit pas  tre utilis    des fins commerciales ou l gales. Les auteurs ne garantissent pas l'exactitude, l'exhaustivit  ou la fiabilit  des informations. Les informations et les donn es utilis es dans ce d p t sont   des fins acad miques et de recherche uniquement. Les donn es obtenues   partir de sources accessibles au public, et les auteurs ne revendiquent aucun droit de propri t  ou de copyright sur les donn es.",documentation
docs/fr/README.md,documentation,5,"ChatTTS est un syst me de synth se vocale puissant. Cependant, il est tr s important d'utiliser cette technologie de mani re responsable et  thique. Pour limiter l'utilisation de ChatTTS, nous avons ajout  une petite quantit  de bruit haute fr quence pendant l'entra nement du mod le de 40 000 heures et compress  la qualit  audio autant que possible en utilisant le format MP3, pour emp cher les acteurs malveillants de l'utiliser potentiellement   des fins criminelles. En m me temps, nous avons entra n  en interne un mod le de d tection et pr voyons de l'open-source   l'avenir.

### Contact
> Les issues/PRs sur GitHub sont toujours les bienvenus.

#### Demandes formelles
Pour les demandes formelles concernant le mod le et la feuille de route, veuillez nous contacter   **open-source@2noise.com**.",documentation
docs/fr/README.md,documentation,6,"#### Discussion en ligne
##### 1. Groupe QQ (application sociale chinoise)
- **Groupe 1**, 808364215 (Complet)
- **Groupe 2**, 230696694 (Complet)
- **Groupe 3**, 933639842 (Complet)
- **Groupe 4**, 608667975

##### 2. Serveur Discord
Rejoignez en cliquant [ici](https://discord.gg/Ud5Jxgx5yD).

## Pour commencer
### Cloner le d p t
```bash
git clone https://github.com/2noise/ChatTTS
cd ChatTTS
```

### Installer les d pendances
#### 1. Installation directe
```bash
pip install --upgrade -r requirements.txt
```

#### 2. Installer depuis conda
```bash
conda create -n chattts
conda activate chattts
pip install -r requirements.txt
```

#### Optionnel : Installer TransformerEngine si vous utilisez un GPU NVIDIA (Linux uniquement)
> [!Note]
> Le processus d'installation est tr s lent.

> [!Warning]
> L'adaptation de TransformerEngine est actuellement en cours de d veloppement et NE PEUT PAS fonctionner correctement pour le moment.
> Installez-le uniquement   des fins de d veloppement.",documentation
docs/fr/README.md,documentation,7,"```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable
```

#### Optionnel : Installer FlashAttention-2 (principalement GPU NVIDIA)
> [!Note]
> Voir les appareils pris en charge dans la [documentation Hugging Face](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).

> [!Warning]
> Actuellement, FlashAttention-2 ralentira la vitesse de g n ration selon [ce probl me](https://github.com/huggingface/transformers/issues/26990). 
> Installez-le uniquement   des fins de d veloppement.

```bash
pip install flash-attn --no-build-isolation
```

### D marrage rapide
> Assurez-vous que vous  tes dans le r pertoire racine du projet lorsque vous ex cutez ces commandes ci-dessous.

#### 1. Lancer WebUI
```bash
python examples/web/webui.py
```

#### 2. Inf rence par ligne de commande
> Cela enregistrera l'audio sous  ./output_audio_n.mp3 

```bash
python examples/cmd/run.py ""Votre premier texte."" ""Votre deuxi me texte.""
```

## Installation",documentation
docs/fr/README.md,documentation,8,"1. Installer la version stable depuis PyPI
```bash
pip install ChatTTS
```

2. Installer la derni re version depuis GitHub
```bash
pip install git+https://github.com/2noise/ChatTTS
```

3. Installer depuis le r pertoire local en mode d veloppement
```bash
pip install -e .
```

### Utilisation de base

```python
import ChatTTS
import torch
import torchaudio

chat = ChatTTS.Chat()
chat.load(compile=False) # D finissez sur True pour de meilleures performances

texts = [""METTEZ VOTRE PREMIER TEXTE ICI"", ""METTEZ VOTRE DEUXI ME TEXTE ICI""]

wavs = chat.infer(texts)

torchaudio.save(""output1.wav"", torch.from_numpy(wavs[0]), 24000)
```

### Utilisation avanc e

```python
###################################
#  chantillonner un locuteur   partir d'une distribution gaussienne.

rand_spk = chat.sample_random_speaker()
print(rand_spk) # sauvegardez-le pour une r cup ration ult rieure du timbre",documentation
docs/fr/README.md,documentation,9,"params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb = rand_spk, # ajouter le locuteur  chantillonn  
    temperature = .3,   # en utilisant une temp rature personnalis e
    top_P = 0.7,        # top P d code
    top_K = 20,         # top K d code
)

###################################
# Pour le contr le manuel au niveau des phrases.

# utilisez oral_(0-9), laugh_(0-2), break_(0-7) 
# pour g n rer un token sp cial dans le texte   synth tiser.
params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_6]',
)

wavs = chat.infer(
    texts,
    params_refine_text=params_refine_text,
    params_infer_code=params_infer_code,
)

###################################
# Pour le contr le manuel au niveau des mots.",documentation
docs/fr/README.md,documentation,10,"text = 'Quel est [uv_break]votre plat anglais pr f r ?[laugh][lbreak]'
wavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)
torchaudio.save(""output2.wav"", torch.from_numpy(wavs[0]), 24000)
```

<details open>
  <summary><h4>Exemple : auto-pr sentation</h4></summary>

```python
inputs_en = """"""
chat T T S est un mod le de synth se vocale con u pour les applications de dialogue.
[uv_break]il prend en charge les entr es en langues mixtes [uv_break]et offre des capacit s multi-locuteurs
avec un contr le pr cis des  l ments prosodiques comme 
[uv_break]le rire[uv_break][laugh], [uv_break]les pauses, [uv_break]et l'intonation.
[uv_break]il d livre une parole naturelle et expressive,[uv_break]donc veuillez
[uv_break]utiliser le projet de mani re responsable   vos risques et p rils.[uv_break]
"""""".replace('\n', '') # L'anglais est encore exp rimental.",documentation
docs/fr/README.md,documentation,11,"params_refine_text = ChatTTS.Chat.RefineTextParams(
    prompt='[oral_2][laugh_0][break_4]',
)

audio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)
torchaudio.save(""output3.wav"", torch.from_numpy(audio_array_en[0]), 24000)
```

<table>
<tr>
<td align=""center"">

**locuteur masculin**

</td>
<td align=""center"">

**locutrice f minine**

</td>
</tr>
<tr>
<td align=""center"">

[locuteur masculin](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)

</td>
<td align=""center"">

[locutrice f minine](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)

</td>
</tr>
</table>

</details>

## FAQ",documentation
docs/fr/README.md,documentation,12,"#### 1. De combien de VRAM ai-je besoin ? Quelle est la vitesse d'inf rence ?
Pour un clip audio de 30 secondes, au moins 4 Go de m moire GPU sont n cessaires. Pour le GPU 4090, il peut g n rer de l'audio correspondant   environ 7 tokens s mantiques par seconde. Le Facteur Temps R el (RTF) est d'environ 0.3.

#### 2. La stabilit  du mod le n'est pas suffisante, avec des probl mes tels que des locuteurs multiples ou une mauvaise qualit  audio.
C'est un probl me qui se produit g n ralement avec les mod les autoregressifs (pour bark et valle). Il est g n ralement difficile    viter. On peut essayer plusieurs  chantillons pour trouver un r sultat appropri .",documentation
docs/fr/README.md,documentation,13,"#### 3. En plus du rire, pouvons-nous contr ler autre chose ? Pouvons-nous contr ler d'autres  motions ?
Dans le mod le actuellement publi , les seules unit s de contr le au niveau des tokens sont `[laugh]`, `[uv_break]`, et `[lbreak]`. Dans les futures versions, nous pourrions open-source des mod les avec des capacit s de contr le  motionnel suppl mentaires.

## Remerciements
- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) et [valle](https://arxiv.org/abs/2301.02111) d montrent un r sultat TTS remarquable par un syst me de style autoregressif.
- [fish-speech](https://github.com/fishaudio/fish-speech) r v le la capacit  de GVQ en tant que tokenizer audio pour la mod lisation LLM.
- [vocos](https://github.com/gemelo-ai/vocos) qui est utilis  comme vocodeur pr -entra n .

## Appr ciation sp ciale
- [wlu-audio lab](https://audio.westlake.edu.cn/) pour les exp riences d'algorithme pr coce.",documentation
docs/fr/README.md,documentation,14,"## Merci   tous les contributeurs pour leurs efforts
[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)

<div align=""center"">

  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)

</div>",documentation
requirements.txt,documentation,0,"numpy<3.0.0
numba
torch>=2.1.0
torchaudio
tqdm
vector_quantize_pytorch
transformers>=4.41.1
vocos
IPython
gradio
pybase16384
pynini==2.1.5; sys_platform == 'linux'
WeTextProcessing; sys_platform == 'linux'
nemo_text_processing; sys_platform == 'linux'
av
pydub",documentation
examples/api/requirements.txt,documentation,0,"fastapi
requests",documentation
setup.py,code,0,"import os
from setuptools import setup, find_packages

version = ""v0.0.0""

setup(
    name=""chattts"",
    version=os.environ.get(""CHTTS_VER"", version).lstrip(""v""),
    description=""A generative speech model for daily dialogue"",
    long_description=open(""README.md"", encoding=""utf8"").read(),
    long_description_content_type=""text/markdown"",
    author=""2noise"",
    author_email=""open-source@2noise.com"",
    maintainer=""fumiama"",
    url=""https://github.com/2noise/ChatTTS"",
    packages=find_packages(include=[""ChatTTS"", ""ChatTTS.*""]),
    package_data={
        ""ChatTTS.res"": [""homophones_map.json"", ""sha256_map.json""],
    },
    license=""AGPLv3+"",
    install_requires=[
        ""numba"",
        ""numpy<3.0.0"",
        ""pybase16384"",
        ""torch>=2.1.0"",
        ""torchaudio"",
        ""tqdm"",
        ""transformers>=4.41.1"",
        ""vector_quantize_pytorch"",
        ""vocos"",
    ],
    platforms=""any"",
    classifiers=[
        ""Programming Language :: Python :: 3"",
        ""Operating System :: OS Independent"",
        ""License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)"",
    ],
)
",code
.gitignore,configuration,0,"# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
*.ckpt
# C extensions
*.so
*.pt

# Distribution / packaging
.Python
outputs/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
asset/*
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.loc",configuration
tools/__init__.py,code,0,,code
ChatTTS/core.py,code,0,"import os
import re
import logging
import tempfile
from dataclasses import dataclass, asdict
from typing import Literal, Optional, List, Tuple, Dict, Union
from json import load
from pathlib import Path

import numpy as np
import torch
from vocos import Vocos
from vocos.pretrained import instantiate_class
from huggingface_hub import snapshot_download

from .config import Config
from .model import DVAE, Embed, GPT, gen_logits, Tokenizer, Speaker
from .utils import (
    load_safetensors,
    check_all_assets,
    download_all_assets,
    select_device,
    get_latest_modified_file,
    del_all,
)
from .utils import logger as utils_logger
from .utils import FileLike

from .norm import Normalizer


class Chat:
    def __init__(self, logger=logging.getLogger(__name__)):
        self.logger = logger
        utils_logger.set_logger(logger)

        self.config = Config()

        self.normalizer = Normalizer(
            os.path.join(os.path.dirname(__file__), ""res"", ""homophones_map.json""),
            logger,
        )
        with open(
            os.path.join(os.path.dirname(__file__), ""res"", ""sha256_map.json"")
        ) as f:
            self.sha256_map: Dict[str, str] = load(f)

        self.context = GPT.Context()

    def has_loaded(self, use_decoder=False):
        not_finish = False
        check_list = [""vocos"", ""gpt"", ""tokenizer"", ""embed""]

        if use_decoder:
            check_list.append(""decoder"")
        else:
            check_list.append(""dvae"")

        for module in check_list:
            if not hasattr(self, module):
                self.logger.warning(f""{module} not initialized."")
                not_finish = True

        return not not_finish

    def download_models(
        self,
        source: Literal[""huggingface"", ""local"", ""custom""] = ""local"",
        force_redownload=False,
        custom_path: Optional[FileLike] = None,
    ) -> Optional[str]:
        if source == ""local"":
            download_path = custom_path if custom_path is not N",code
ChatTTS/norm.py,code,0,"import json
import logging
import re
from typing import Dict, Tuple, List, Literal, Callable, Optional
import sys

from numba import jit
import numpy as np

from .utils import del_all


@jit(nopython=True)
def _find_index(table: np.ndarray, val: np.uint16):
    for i in range(table.size):
        if table[i] == val:
            return i
    return -1


@jit(nopython=True)
def _fast_replace(
    table: np.ndarray, text: bytes
) -> Tuple[np.ndarray, List[Tuple[str, str]]]:
    result = np.frombuffer(text, dtype=np.uint16).copy()
    replaced_words = []
    for i in range(result.size):
        ch = result[i]
        p = _find_index(table[0], ch)
        if p >= 0:
            repl_char = table[1][p]
            result[i] = repl_char
            replaced_words.append((chr(ch), chr(repl_char)))
    return result, replaced_words


@jit(nopython=True)
def _split_tags(text: str) -> Tuple[List[str], List[str]]:
    texts: List[str] = []
    tags: List[str] = []
    current_text = """"
    current_tag = """"
    for c in text:
        if c == ""["":
            texts.append(current_text)
            current_text = """"
            current_tag = c
        elif current_tag != """":
            current_tag += c
        else:
            current_text += c
        if c == ""]"":
            tags.append(current_tag)
            current_tag = """"
    if current_text != """":
        texts.append(current_text)
    return texts, tags


@jit(nopython=True)
def _combine_tags(texts: List[str], tags: List[str]) -> str:
    text = """"
    for t in texts:
        tg = """"
        if len(tags) > 0:
            tg = tags.pop(0)
        text += t + tg
    return text


class Normalizer:
    def __init__(self, map_file_path: str, logger=logging.getLogger(__name__)):
        self.logger = logger
        self.normalizers: Dict[str, Callable[[str], str]] = {}
        self.homophones_map = self._load_homophones_map(map_file_path)
        """"""
        homophones_map

        Replace the mispronounced characters with ",code
ChatTTS/__init__.py,code,0,"from .core import Chat
",code
tests/#655.py,code,0,"import os, sys

if sys.platform == ""darwin"":
    os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging

import torch

import ChatTTS

from tools.logger import get_logger
from tools.normalizer import normalizer_en_nemo_text

logger = get_logger(""Test"", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source=""huggingface"")  # Set to True for better performance
try:
    chat.normalizer.register(""en"", normalizer_en_nemo_text())
except:
    logger.warning(""Package nemo_text_processing not found!"")

rand_spk = chat.sample_random_speaker()


text = [""What is [uv_break]your favorite english food?[laugh][lbreak]""]

fail = False

refined_text = chat.infer(
    text,
    refine_text_only=True,
    params_refine_text=ChatTTS.Chat.RefineTextParams(
        prompt=""[oral_2][laugh_0][break_6]"",
        manual_seed=12345,
    ),
    split_text=False,
)
if refined_text[0] not in [
    ""what is [uv_break] your favorite english [uv_break] food [laugh] like [lbreak]"",
    ""like what is [uv_break] your favorite english food [laugh] [lbreak]"",
]:
    fail = True
    logger.warning(""refined text is '%s'"", refined_text[0])

params = ChatTTS.Chat.InferCodeParams(
    spk_emb=rand_spk,  # add sampled speaker
    temperature=0.3,  # using custom temperature
    top_P=0.7,  # top P decode
    top_K=20,  # top K decode
)
input_ids, attention_mask, text_mask = chat.tokenizer.encode(
    chat.speaker.decorate_code_prompts(
        text,
        params.prompt,
        params.txt_smp,
        params.spk_emb,
    ),
    chat.config.gpt.num_vq,
    prompt=(
        chat.speaker.decode_prompt(params.spk_smp)
        if params.spk_smp is not None
        else None
    ),
    device=chat.device_gpt,
)
with torch.inference_mode():
    start_idx, end_idx = 0, torch.zeros(
        input_ids.shape[0], device=input_ids.device, dtype=torch.long
    ).fill_(input_ids.shape[1])

    recoded_text = chat.tokenizer.decode(
        chat",code
tests/#511.py,code,0,"import os, sys

if sys.platform == ""darwin"":
    os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging

import ChatTTS

from tools.logger import get_logger

logger = get_logger(""Test"", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source=""huggingface"")  # Set to True for better performance

texts = [
    ""                                      [uv_break]"",
    ""                                   [uv_break]     "",
    ""              [uv_break]                         "",
    ""                   [uv_break]       seed id                "",
    ""d id       id [uv_break]                      pt      "",
    ""                              [uv_break]   "",
    ""               [uv_break]                          "",
    ""                                          pr [uv_break] "",
]

params_infer_code = ChatTTS.Chat.InferCodeParams(
    spk_emb=chat.sample_random_speaker(),
    temperature=0.3,
    top_P=0.005,
    top_K=1,
    show_tqdm=False,
)

fail = False

wavs = chat.infer(
    texts,
    skip_refine_text=True,
    split_text=False,
    params_infer_code=params_infer_code,
)

for k, wav in enumerate(wavs):
    if wav is None:
        logger.warning(""index"", k, ""is None"")
        fail = True

if fail:
    import sys

    sys.exit(1)
",code
tests/#588.py,code,0,"import os, sys

if sys.platform == ""darwin"":
    os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""

now_dir = os.getcwd()
sys.path.append(now_dir)

import logging
import re

import ChatTTS

from tools.logger import get_logger

logger = get_logger(""Test"", lv=logging.WARN)

chat = ChatTTS.Chat(logger)
chat.load(compile=False, source=""huggingface"")  # Set to True for better performance

texts = [
    ""AI AgentAIAGI"",
    """",
]

fail = False

refined = chat.infer(
    texts,
    refine_text_only=True,
    stream=False,
    split_text=False,
    params_refine_text=ChatTTS.Chat.RefineTextParams(show_tqdm=False),
)

trimre = re.compile(""\\[[\w_]+\\]"")


def trim_tags(txt: str) -> str:
    global trimre
    return trimre.sub("""", txt)


for i, t in enumerate(refined):
    if len(trim_tags(t)) > 4 * len(texts[i]):
        fail = True
        logger.warning(""in: %s, out: %s"", texts[i], t)

if fail:
    import sys

    sys.exit(1)
",code
examples/__init__.py,code,0,,code
tools/llm/llm.py,code,0,"from openai import OpenAI

prompt_dict = {
    ""kimi"": [
        {
            ""role"": ""system"",
            ""content"": "" Kimi Moonshot AI "",
        },
        {
            ""role"": ""user"",
            ""content"": ""TTS100"",
        },
        {
            ""role"": ""assistant"",
            ""content"": "" , "",
        },
    ],
    ""deepseek"": [
        {""role"": ""system"", ""content"": ""You are a helpful assistant""},
        {
            ""role"": ""user"",
            ""content"": ""TTS100"",
        },
        {
            ""role"": ""assistant"",
            ""content"": "" , "",
        },
    ],
    ""deepseek_TN"": [
        {""role"": ""system"", ""content"": ""You are a helpful assistant""},
        {
            ""role"": ""user"",
            ""content"": ""TTS"",
        },
        {
            ""role"": ""assistant"",
            ""content"": ""TTStext normalization"",
        },
        {""role"": ""user"", ""content"": ""We paid $123 for this desk.""},
        {
            ""role"": ""assistant"",
            ""content"": ""We paid one hundred and twenty three dollars for this desk."",
        },
        {""role"": ""user"", ""content"": ""010-724654""},
        {""role"": ""assistant"", ""content"": """"},
        {""role"": ""user"", ""content"": ""7246000""},
        {
            ""role"": ""assistant"",
            ""content"": """",
        },
    ],
}


class ChatOpenAI:
    def __init__(self, api_key, base_url, model):
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        self.model = model",code
tools/llm/__init__.py,code,0,"from .llm import ChatOpenAI
",code
tools/normalizer/en.py,code,0,"from typing import Callable
from functools import partial


def normalizer_en_nemo_text() -> Callable[[str], str]:
    from nemo_text_processing.text_normalization.normalize import Normalizer

    return partial(
        Normalizer(input_case=""cased"", lang=""en"").normalize,
        verbose=False,
        punct_post_process=True,
    )
",code
tools/normalizer/zh.py,code,0,"from typing import Callable


def normalizer_zh_tn() -> Callable[[str], str]:
    from tn.chinese.normalizer import Normalizer

    return Normalizer(remove_interjections=False).normalize
",code
tools/normalizer/__init__.py,code,0,"from .en import normalizer_en_nemo_text
from .zh import normalizer_zh_tn
",code
tools/audio/np.py,code,0,"import math

import numpy as np
from numba import jit


@jit(nopython=True)
def float_to_int16(audio: np.ndarray) -> np.ndarray:
    am = int(math.ceil(float(np.abs(audio).max())) * 32768)
    am = 32767 * 32768 // am
    return np.multiply(audio, am).astype(np.int16)
",code
tools/audio/ffmpeg.py,code,0,"from pydub.utils import which


def has_ffmpeg_installed() -> bool:
    return which(""ffmpeg"") and which(""ffprobe"")
",code
tools/audio/pcm.py,code,0,"import wave
from io import BytesIO
import numpy as np
from .np import float_to_int16
from .av import wav2


def _pcm_to_wav_buffer(wav: np.ndarray, sample_rate: int = 24000) -> BytesIO:
    """"""
    Convert PCM audio data to a WAV format byte stream (internal utility function).

    :param wav: PCM data, NumPy array, typically in float32 format.
    :param sample_rate: Sample rate (in Hz), defaults to 24000.
    :return: WAV format byte stream, stored in a BytesIO object.
    """"""
    # Create an in-memory byte stream buffer
    buf = BytesIO()

    # Open a WAV file stream in write mode
    with wave.open(buf, ""wb"") as wf:
        # Set number of channels to 1 (mono)
        wf.setnchannels(1)
        # Set sample width to 2 bytes (16-bit)
        wf.setsampwidth(2)
        # Set sample rate
        wf.setframerate(sample_rate)
        # Convert PCM to 16-bit integer and write
        wf.writeframes(float_to_int16(wav))

    # Reset buffer pointer to the beginning
    buf.seek(0, 0)
    return buf


def pcm_arr_to_mp3_view(wav: np.ndarray, sample_rate: int = 24000) -> memoryview:
    """"""
    Convert PCM audio data to MP3 format.

    :param wav: PCM data, NumPy array, typically in float32 format.
    :param sample_rate: Sample rate (in Hz), defaults to 24000.
    :return: MP3 format byte data, returned as a memoryview.
    """"""
    # Get WAV format byte stream
    buf = _pcm_to_wav_buffer(wav, sample_rate)

    # Create output buffer
    buf2 = BytesIO()
    # Convert WAV data to MP3
    wav2(buf, buf2, ""mp3"")
    # Return MP3 data
    return buf2.getbuffer()


def pcm_arr_to_ogg_view(wav: np.ndarray, sample_rate: int = 24000) -> memoryview:
    """"""
    Convert PCM audio data to OGG format (using Vorbis encoding).

    :param wav: PCM data, NumPy array, typically in float32 format.
    :param sample_rate: Sample rate (in Hz), defaults to 24000.
    :return: OGG format byte data, returned as a memoryview.
    """"""
    # Get WAV format byte stream
    buf = _pcm_to_wav_b",code
tools/audio/av.py,code,0,"from io import BufferedWriter, BytesIO
from pathlib import Path
from typing import Dict, Tuple, Optional, Union, List

import av
from av.audio.frame import AudioFrame
from av.audio.resampler import AudioResampler
import numpy as np


video_format_dict: Dict[str, str] = {
    ""m4a"": ""mp4"",
}

audio_format_dict: Dict[str, str] = {
    ""ogg"": ""libvorbis"",
    ""mp4"": ""aac"",
}


def wav2(i: BytesIO, o: BufferedWriter, format: str):
    """"""
    https://github.com/fumiama/Retrieval-based-Voice-Conversion-WebUI/blob/412a9950a1e371a018c381d1bfb8579c4b0de329/infer/lib/audio.py#L20
    """"""
    inp = av.open(i, ""r"")
    format = video_format_dict.get(format, format)
    out = av.open(o, ""w"", format=format)
    format = audio_format_dict.get(format, format)

    ostream = out.add_stream(format)

    for frame in inp.decode(audio=0):
        for p in ostream.encode(frame):
            out.mux(p)

    for p in ostream.encode(None):
        out.mux(p)

    out.close()
    inp.close()


def load_audio(
    file: Union[str, BytesIO, Path],
    sr: Optional[int] = None,
    format: Optional[str] = None,
    mono=True,
) -> Union[np.ndarray, Tuple[np.ndarray, int]]:
    """"""
    https://github.com/fumiama/Retrieval-based-Voice-Conversion-WebUI/blob/412a9950a1e371a018c381d1bfb8579c4b0de329/infer/lib/audio.py#L39
    """"""
    if (isinstance(file, str) and not Path(file).exists()) or (
        isinstance(file, Path) and not file.exists()
    ):
        raise FileNotFoundError(f""File not found: {file}"")
    rate = 0

    container = av.open(file, format=format)
    audio_stream = next(s for s in container.streams if s.type == ""audio"")
    channels = 1 if audio_stream.layout == ""mono"" else 2
    container.seek(0)
    resampler = (
        AudioResampler(format=""fltp"", layout=audio_stream.layout, rate=sr)
        if sr is not None
        else None
    )

    # Estimated maximum total number of samples to pre-allocate the array
    # AV stores length in microseconds by default
    estimated_to",code
tools/audio/__init__.py,code,0,"from .av import load_audio
from .pcm import pcm_arr_to_mp3_view, pcm_arr_to_ogg_view, pcm_arr_to_wav_view
from .ffmpeg import has_ffmpeg_installed
from .np import float_to_int16
",code
tools/seeder/ctx.py,code,0,"import torch


class TorchSeedContext:
    def __init__(self, seed):
        self.seed = seed
        self.state = None

    def __enter__(self):
        self.state = torch.random.get_rng_state()
        torch.manual_seed(self.seed)

    def __exit__(self, type, value, traceback):
        torch.random.set_rng_state(self.state)
",code
tools/seeder/__init__.py,code,0,"from .ctx import TorchSeedContext
",code
tools/logger/log.py,code,0,"import platform, sys
import logging
from datetime import datetime, timezone

logging.getLogger(""numba"").setLevel(logging.WARNING)
logging.getLogger(""httpx"").setLevel(logging.WARNING)
logging.getLogger(""wetext-zh_normalizer"").setLevel(logging.WARNING)
logging.getLogger(""NeMo-text-processing"").setLevel(logging.WARNING)

# from https://github.com/FloatTech/ZeroBot-Plugin/blob/c70766a989698452e60e5e48fb2f802a2444330d/console/console_windows.go#L89-L96
colorCodePanic = ""\x1b[1;31m""
colorCodeFatal = ""\x1b[1;31m""
colorCodeError = ""\x1b[31m""
colorCodeWarn = ""\x1b[33m""
colorCodeInfo = ""\x1b[37m""
colorCodeDebug = ""\x1b[32m""
colorCodeTrace = ""\x1b[36m""
colorReset = ""\x1b[0m""

log_level_color_code = {
    logging.DEBUG: colorCodeDebug,
    logging.INFO: colorCodeInfo,
    logging.WARN: colorCodeWarn,
    logging.ERROR: colorCodeError,
    logging.FATAL: colorCodeFatal,
}

log_level_msg_str = {
    logging.DEBUG: ""DEBU"",
    logging.INFO: ""INFO"",
    logging.WARN: ""WARN"",
    logging.ERROR: ""ERRO"",
    logging.FATAL: ""FATL"",
}


class Formatter(logging.Formatter):
    def __init__(self, color=platform.system().lower() != ""windows""):
        # https://stackoverflow.com/questions/2720319/python-figure-out-local-timezone
        self.tz = datetime.now(timezone.utc).astimezone().tzinfo
        self.color = color

    def format(self, record: logging.LogRecord):
        logstr = ""["" + datetime.now(self.tz).strftime(""%z %Y%m%d %H:%M:%S"") + ""] [""
        if self.color:
            logstr += log_level_color_code.get(record.levelno, colorCodeInfo)
        logstr += log_level_msg_str.get(record.levelno, record.levelname)
        if self.color:
            logstr += colorReset
        if sys.version_info >= (3, 9):
            fn = record.filename.removesuffix("".py"")
        elif record.filename.endswith("".py""):
            fn = record.filename[:-3]
        logstr += f""] {str(record.name)} | {fn} | {str(record.msg)%record.args}""
        return logstr


def get_logger(name: str, lv=logging.",code
tools/logger/__init__.py,code,0,"from .log import get_logger
",code
ChatTTS/res/__init__.py,code,0,,code
ChatTTS/model/gpt.py,code,0,"import platform
from dataclasses import dataclass
import logging
from typing import Union, List, Optional, Tuple, Callable
import gc

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.parametrize as P
from tqdm import tqdm
from transformers import LlamaModel, LlamaConfig
from transformers.cache_utils import Cache
from transformers.modeling_outputs import BaseModelOutputWithPast
from transformers.utils import is_flash_attn_2_available

from ..utils import del_all
from .embed import Embed


class GPT(nn.Module):
    def __init__(
        self,
        gpt_config: dict,
        embed: Embed,
        use_flash_attn=False,
        use_vllm=False,
        device=torch.device(""cpu""),
        device_gpt=torch.device(""cpu""),
        logger=logging.getLogger(__name__),
    ):
        super().__init__()

        self.logger = logger

        self.device = device
        self.device_gpt = device_gpt

        self.generator = torch.Generator(device=device)

        self.num_vq = int(gpt_config[""num_vq""])
        self.num_audio_tokens = int(gpt_config[""num_audio_tokens""])
        self.num_text_tokens = int(gpt_config[""num_text_tokens""])

        self.use_flash_attn = use_flash_attn
        self.is_te_llama = False
        self.is_vllm = use_vllm

        if self.is_vllm:
            return

        self.llama_config = self._build_llama_config(gpt_config)

        self.emb_code = [ec.__call__ for ec in embed.emb_code]
        self.emb_text = embed.emb_text.__call__
        self.head_text = embed.head_text.__call__
        self.head_code = [hc.__call__ for hc in embed.head_code]

    def load_pretrained(
        self, gpt_folder: str, embed_file_path: str, experimental=False
    ):
        if self.is_vllm and platform.system().lower() == ""linux"":

            from .velocity import LLM

            self.llm = LLM(
                model=gpt_folder,
                num_audio_tokens=self.num_audio_tokens,
                num_text_tokens=self.num_t",code
ChatTTS/model/dvae.py,code,0,"import math
from typing import List, Optional, Literal, Union

import numpy as np
import pybase16384 as b14
import torch
import torch.nn as nn
import torchaudio
from vector_quantize_pytorch import GroupedResidualFSQ

from ..utils import load_safetensors


class ConvNeXtBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        intermediate_dim: int,
        kernel: int,
        dilation: int,
        layer_scale_init_value: float = 1e-6,
    ):
        # ConvNeXt Block copied from Vocos.
        super().__init__()
        self.dwconv = nn.Conv1d(
            dim,
            dim,
            kernel_size=kernel,
            padding=dilation * (kernel // 2),
            dilation=dilation,
            groups=dim,
        )  # depthwise conv

        self.norm = nn.LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(
            dim, intermediate_dim
        )  # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(intermediate_dim, dim)
        self.weight = (
            nn.Parameter(layer_scale_init_value * torch.ones(dim), requires_grad=True)
            if layer_scale_init_value > 0
            else None
        )

    def forward(self, x: torch.Tensor, cond=None) -> torch.Tensor:
        residual = x

        y = self.dwconv(x)
        y.transpose_(1, 2)  # (B, C, T) -> (B, T, C)
        x = self.norm(y)
        del y
        y = self.pwconv1(x)
        del x
        x = self.act(y)
        del y
        y = self.pwconv2(x)
        del x
        if self.weight is not None:
            y *= self.weight
        y.transpose_(1, 2)  # (B, T, C) -> (B, C, T)

        x = y + residual
        del y

        return x


class GFSQ(nn.Module):

    def __init__(
        self, dim: int, levels: List[int], G: int, R: int, eps=1e-5, transpose=True
    ):
        super(GFSQ, self).__init__()
        self.quantizer = GroupedResidualFSQ(
            dim=dim,
            levels=list(levels),
     ",code
ChatTTS/model/embed.py,code,0,"import torch
import torch.nn as nn
from torch.nn.utils.parametrizations import weight_norm

from ..utils import load_safetensors


class Embed(nn.Module):
    def __init__(
        self, hidden_size: int, num_audio_tokens: int, num_text_tokens: int, num_vq=4
    ):
        super().__init__()

        self.num_vq = num_vq
        self.num_audio_tokens = num_audio_tokens

        self.model_dim = hidden_size
        self.emb_code = nn.ModuleList(
            [nn.Embedding(num_audio_tokens, self.model_dim) for _ in range(num_vq)],
        )
        self.emb_text = nn.Embedding(num_text_tokens, self.model_dim)

        self.head_text = weight_norm(
            nn.Linear(self.model_dim, num_text_tokens, bias=False),
            name=""weight"",
        )
        self.head_code = nn.ModuleList(
            [
                weight_norm(
                    nn.Linear(self.model_dim, num_audio_tokens, bias=False),
                    name=""weight"",
                )
                for _ in range(self.num_vq)
            ],
        )

    @torch.inference_mode()
    def load_pretrained(self, filename: str, device: torch.device):
        state_dict_tensors = load_safetensors(filename)
        self.load_state_dict(state_dict_tensors)
        self.to(device)

    def __call__(
        self, input_ids: torch.Tensor, text_mask: torch.Tensor
    ) -> torch.Tensor:
        """"""
        get_emb
        """"""
        return super().__call__(input_ids, text_mask)

    @torch.inference_mode()
    def forward(self, input_ids: torch.Tensor, text_mask: torch.Tensor) -> torch.Tensor:
        """"""
        get_emb
        """"""
        device = next(self.parameters()).device
        emb_text: torch.Tensor = self.emb_text(
            input_ids[text_mask].narrow(1, 0, 1).squeeze_(1).to(device)
        )

        text_mask_inv = text_mask.logical_not().to(device)
        masked_input_ids: torch.Tensor = input_ids[text_mask_inv].to(device)

        emb_code = [
            self.emb_code[i](masked_inpu",code
ChatTTS/model/processors.py,code,0,"import torch
import torch.nn.functional as F
from transformers.generation import TopKLogitsWarper, TopPLogitsWarper


class CustomRepetitionPenaltyLogitsProcessorRepeat:

    def __init__(self, penalty: float, max_input_ids: int, past_window: int):
        if not isinstance(penalty, float) or not (penalty > 0):
            raise ValueError(
                f""`penalty` has to be a strictly positive float, but is {penalty}""
            )

        self.penalty = penalty
        self.max_input_ids = max_input_ids
        self.past_window = past_window

    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
    ) -> torch.FloatTensor:
        if input_ids.size(1) > self.past_window:
            input_ids = input_ids.narrow(1, -self.past_window, self.past_window)
        freq = F.one_hot(input_ids, scores.size(1)).sum(1)
        if freq.size(0) > self.max_input_ids:
            freq.narrow(
                0, self.max_input_ids, freq.size(0) - self.max_input_ids
            ).zero_()
        alpha = torch.pow(self.penalty, freq)
        scores = scores.contiguous()
        inp = scores.multiply(alpha)
        oth = scores.divide(alpha)
        con = scores < 0
        out = torch.where(con, inp, oth)
        del inp, oth, scores, con, alpha
        return out


def gen_logits(
    num_code: int,
    top_P=0.7,
    top_K=20,
    repetition_penalty=1.0,
):
    logits_warpers = []
    if top_P is not None:
        logits_warpers.append(TopPLogitsWarper(top_P, min_tokens_to_keep=3))
    if top_K is not None:
        logits_warpers.append(TopKLogitsWarper(top_K, min_tokens_to_keep=3))

    logits_processors = []
    if repetition_penalty is not None and repetition_penalty != 1:
        logits_processors.append(
            CustomRepetitionPenaltyLogitsProcessorRepeat(
                repetition_penalty, num_code, 16
            )
        )

    return logits_warpers, logits_processors
",code
ChatTTS/model/tokenizer.py,code,0,"import os

os.environ[""TOKENIZERS_PARALLELISM""] = ""false""
""""""
https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning
""""""

from typing import List, Tuple, Optional, Union

import torch
from transformers import BertTokenizerFast

from ..utils import del_all, FileLike


class Tokenizer:
    def __init__(
        self,
        tokenizer_path: FileLike,
    ):
        """"""
        tokenizer: BertTokenizerFast = torch.load(
            tokenizer_path, map_location=device, mmap=True
        )
        # tokenizer.save_pretrained(""asset/tokenizer"", legacy_format=False)
        """"""
        tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(tokenizer_path)
        self._tokenizer = tokenizer

        self.len = len(tokenizer)
        self.spk_emb_ids = tokenizer.convert_tokens_to_ids(""[spk_emb]"")
        self.break_0_ids = tokenizer.convert_tokens_to_ids(""[break_0]"")
        self.eos_token = tokenizer.convert_tokens_to_ids(""[Ebreak]"")

    @torch.inference_mode()
    def encode(
        self,
        text: List[str],
        num_vq: int,
        prompt: Optional[torch.Tensor] = None,
        device=""cpu"",
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:

        input_ids_lst = []
        attention_mask_lst = []
        max_input_ids_len = -1
        max_attention_mask_len = -1
        prompt_size = 0

        if prompt is not None:
            assert prompt.size(0) == num_vq, ""prompt dim 0 must equal to num_vq""
            prompt_size = prompt.size(1)

        # avoid random speaker embedding of tokenizer in the other dims
        for t in text:
            x = self._tokenizer.encode_plus(
                t, return_tensors=""pt"", add_special_tokens=False, padding=True
            )
            input_ids_lst.append(x[""input_ids""].squeeze_(0))
            attention_mask_lst.append(x[""attention_mask""].squeeze_(0))
            del_all(x)
            ids_sz = input_ids_lst[-1].size(0)
            if ids_sz > max_i",code
ChatTTS/model/speaker.py,code,0,"import lzma
from typing import List, Optional, Union

import pybase16384 as b14
import numpy as np
import torch
import torch.nn.functional as F


class Speaker:
    def __init__(self, dim: int, spk_cfg: str, device=torch.device(""cpu"")) -> None:
        spk_stat = torch.from_numpy(
            np.frombuffer(b14.decode_from_string(spk_cfg), dtype=np.float16).copy()
        ).to(device=device)
        self.std, self.mean = spk_stat.requires_grad_(False).chunk(2)
        self.dim = dim

    def sample_random(self) -> str:
        return self._encode(self._sample_random())

    @torch.inference_mode()
    def apply(
        self,
        emb: torch.Tensor,
        spk_emb: Union[str, torch.Tensor],
        input_ids: torch.Tensor,
        spk_emb_ids: int,
        device: torch.device,
        inplace: bool = True,
    ) -> torch.Tensor:
        if isinstance(spk_emb, str):
            spk_emb_tensor = torch.from_numpy(self._decode(spk_emb))
        else:
            spk_emb_tensor = spk_emb
        n = (
            F.normalize(
                spk_emb_tensor,
                p=2.0,
                dim=0,
                eps=1e-12,
            )
            .to(device)
            .unsqueeze_(0)
            .expand(emb.size(0), -1)
            .unsqueeze_(1)
            .expand(emb.shape)
        )
        cond = input_ids.narrow(-1, 0, 1).eq(spk_emb_ids).expand(emb.shape)
        out = torch.where(cond, n, emb, out=emb if inplace else None)
        if inplace:
            del cond, n
        return out

    @staticmethod
    @torch.no_grad()
    def decorate_code_prompts(
        text: List[str],
        prompt: str,
        txt_smp: Optional[str],
        spk_emb: Optional[str],
    ) -> List[str]:
        for i, t in enumerate(text):
            text[i] = (
                t.replace(""[Stts]"", """")
                .replace(""[spk_emb]"", """")
                .replace(""[empty_spk]"", """")
                .strip()
            )
            """"""
            see https://github.c",code
ChatTTS/model/__init__.py,code,0,"from .dvae import DVAE
from .embed import Embed
from .gpt import GPT
from .processors import gen_logits
from .speaker import Speaker
from .tokenizer import Tokenizer
",code
ChatTTS/config/config.py,code,0,"from dataclasses import dataclass


@dataclass(repr=False, eq=False)
class Path:
    vocos_ckpt_path: str = ""asset/Vocos.safetensors""
    dvae_ckpt_path: str = ""asset/DVAE.safetensors""
    gpt_ckpt_path: str = ""asset/gpt""
    decoder_ckpt_path: str = ""asset/Decoder.safetensors""
    tokenizer_path: str = ""asset/tokenizer""
    embed_path: str = ""asset/Embed.safetensors""


@dataclass(repr=False, eq=False)
class Decoder:
    idim: int = 384
    odim: int = 384
    hidden: int = 512
    n_layer: int = 12
    bn_dim: int = 128


@dataclass(repr=False, eq=False)
class VQ:
    dim: int = 1024
    levels: tuple = (5, 5, 5, 5)
    G: int = 2
    R: int = 2


@dataclass(repr=False, eq=False)
class DVAE:
    encoder: Decoder = Decoder(
        idim=512,
        odim=1024,
        hidden=256,
        n_layer=12,
        bn_dim=128,
    )
    decoder: Decoder = Decoder(
        idim=512,
        odim=512,
        hidden=256,
        n_layer=12,
        bn_dim=128,
    )
    vq: VQ = VQ()


@dataclass(repr=False, eq=False)
class GPT:
    hidden_size: int = 768
    intermediate_size: int = 3072
    num_attention_heads: int = 12
    num_hidden_layers: int = 20
    use_cache: bool = False
    max_position_embeddings: int = 4096

    spk_emb_dim: int = 192
    spk_KL: bool = False
    num_audio_tokens: int = 626
    num_text_tokens: int = 21178
    num_vq: int = 4


@dataclass(repr=False, eq=False)
class Embed:
    hidden_size: int = 768
    num_audio_tokens: int = 626
    num_text_tokens: int = 21178
    num_vq: int = 4


@dataclass(repr=False, eq=False)
class FeatureExtractorInitArgs:
    sample_rate: int = 24000
    n_fft: int = 1024
    hop_length: int = 256
    n_mels: int = 100
    padding: str = ""center""


@dataclass(repr=False, eq=False)
class FeatureExtractor:
    class_path: str = ""vocos.feature_extractors.MelSpectrogramFeatures""
    init_args: FeatureExtractorInitArgs = FeatureExtractorInitArgs()


@dataclass(repr=False, eq=False)
class BackboneInitArgs:
    input_channels:",code
ChatTTS/config/__init__.py,code,0,"from .config import Config
",code
ChatTTS/utils/log.py,code,0,"import logging
from pathlib import Path


class Logger:
    def __init__(self, logger=logging.getLogger(Path(__file__).parent.name)):
        self.logger = logger

    def set_logger(self, logger: logging.Logger):
        self.logger = logger

    def get_logger(self) -> logging.Logger:
        return self.logger


logger = Logger()
",code
ChatTTS/utils/gpu.py,code,0,"import torch

try:
    import torch_npu
except ImportError:
    pass

from .log import logger


def select_device(min_memory=2047, experimental=False):
    has_cuda = torch.cuda.is_available()
    if has_cuda or _is_torch_npu_available():
        provider = torch.cuda if has_cuda else torch.npu
        """"""
        Using Ascend NPU to accelerate the process of inferencing when GPU is not found.
        """"""
        dev_idx = 0
        max_free_memory = -1
        for i in range(provider.device_count()):
            props = provider.get_device_properties(i)
            free_memory = props.total_memory - provider.memory_reserved(i)
            if max_free_memory < free_memory:
                dev_idx = i
                max_free_memory = free_memory
        free_memory_mb = max_free_memory / (1024 * 1024)
        if free_memory_mb < min_memory:
            logger.get_logger().warning(
                f""{provider.device(dev_idx)} has {round(free_memory_mb, 2)} MB memory left. Switching to CPU.""
            )
            device = torch.device(""cpu"")
        else:
            device = provider._get_device(dev_idx)
    elif torch.backends.mps.is_available():
        """"""
        Currently MPS is slower than CPU while needs more memory and core utility,
        so only enable this for experimental use.
        """"""
        if experimental:
            # For Apple M1/M2 chips with Metal Performance Shaders
            logger.get_logger().warning(""experimantal: found apple GPU, using MPS."")
            device = torch.device(""mps"")
        else:
            logger.get_logger().info(""found Apple GPU, but use CPU."")
            device = torch.device(""cpu"")
    else:
        logger.get_logger().warning(""no GPU or NPU found, use CPU instead"")
        device = torch.device(""cpu"")

    return device


def _is_torch_npu_available():
    try:
        # will raise a AttributeError if torch_npu is not imported or a RuntimeError if no NPU found
        _ = torch.npu.device_count()
        r",code
ChatTTS/utils/io.py,code,0,"import os
import logging
from typing import Union, IO
from dataclasses import is_dataclass

from safetensors import safe_open
import torch

from .log import logger

if hasattr(torch.serialization, ""FILE_LIKE""):
    FileLike = torch.serialization.FILE_LIKE
elif hasattr(torch.types, ""FILE_LIKE""):
    FileLike = torch.types.FileLike
else:
    FileLike = Union[str, os.PathLike, IO[bytes]]


@torch.inference_mode()
def load_safetensors(filename: str):
    state_dict_tensors = {}
    with safe_open(filename, framework=""pt"") as f:
        for k in f.keys():
            state_dict_tensors[k] = f.get_tensor(k)
    return state_dict_tensors


def get_latest_modified_file(directory):

    files = [os.path.join(directory, f) for f in os.listdir(directory)]
    if not files:
        logger.get_logger().log(
            logging.WARNING, f""no files found in the directory: {directory}""
        )
        return None
    latest_file = max(files, key=os.path.getmtime)

    return latest_file


def del_all(d: Union[dict, list]):
    if is_dataclass(d):
        for k in list(vars(d).keys()):
            x = getattr(d, k)
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
            delattr(d, k)
    elif isinstance(d, dict):
        lst = list(d.keys())
        for k in lst:
            x = d.pop(k)
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
    elif isinstance(d, list):
        while len(d):
            x = d.pop()
            if isinstance(x, dict) or isinstance(x, list) or is_dataclass(x):
                del_all(x)
            del x
    else:
        del d
",code
ChatTTS/utils/dl.py,code,0,"import os
from pathlib import Path
import hashlib
import requests
from io import BytesIO
from typing import Dict, Tuple, Optional
from mmap import mmap, ACCESS_READ

from .log import logger


def sha256(fileno: int) -> str:
    data = mmap(fileno, 0, access=ACCESS_READ)
    h = hashlib.sha256(data).hexdigest()
    del data
    return h


def check_model(
    dir_name: Path, model_name: str, hash: str, remove_incorrect=False
) -> bool:
    target = dir_name / model_name
    relname = target.as_posix()
    logger.get_logger().debug(f""checking {relname}..."")
    if not os.path.exists(target):
        logger.get_logger().info(f""{target} not exist."")
        return False
    with open(target, ""rb"") as f:
        digest = sha256(f.fileno())
        bakfile = f""{target}.bak""
        if digest != hash:
            logger.get_logger().warning(f""{target} sha256 hash mismatch."")
            logger.get_logger().info(f""expected: {hash}"")
            logger.get_logger().info(f""real val: {digest}"")
            if remove_incorrect:
                if not os.path.exists(bakfile):
                    os.rename(str(target), bakfile)
                else:
                    os.remove(str(target))
            return False
        if remove_incorrect and os.path.exists(bakfile):
            os.remove(bakfile)
    return True


def check_folder(
    base_dir: Path,
    *innder_dirs: str,
    names: Tuple[str],
    sha256_map: Dict[str, str],
    update=False,
) -> bool:
    key = ""sha256_""
    current_dir = base_dir
    for d in innder_dirs:
        current_dir /= d
        key += f""{d}_""

    for model in names:
        menv = model.replace(""."", ""_"")
        if not check_model(current_dir, model, sha256_map[f""{key}{menv}""], update):
            return False
    return True


def check_all_assets(base_dir: Path, sha256_map: Dict[str, str], update=False) -> bool:
    logger.get_logger().info(""checking assets..."")

    if not check_folder(
        base_dir,
        ""asset"",
        names=(",code
ChatTTS/utils/__init__.py,code,0,"from .dl import check_all_assets, download_all_assets
from .gpu import select_device
from .io import load_safetensors, get_latest_modified_file, del_all, FileLike
from .log import logger
",code
ChatTTS/model/velocity/llama.py,code,0,"# coding=utf-8
# Adapted from
# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py
# Copyright 2023 The vLLM team.
# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Inference-only LLaMA model compatible with HuggingFace weights.""""""
from typing import Any, Dict, List, Optional, Tuple

import torch
from torch import nn
from transformers import LlamaConfig

from vllm.model_executor.input_metadata import InputMetadata
from vllm.model_executor.layers.activation import SiluAndMul
from vllm.model_executor.layers.attention import PagedAttention
from vllm.model_executor.layers.layernorm import RMSNorm
from vllm.model_executor.layers.linear import (
    LinearMethodBase,
    MergedColumnParallelLinear,
    QKVParallelLinear,
    RowParallelLinear,
)
from vllm.model_executor.layers.rotary_embedding import get_rope
from vllm.model_executor.layers.sampler import Sampler
from vllm.model_executor.layers.vocab_parallel_embedding import (
    VocabParallelEmbedding,
    ParallelLMHead,
)
from vllm.model_executor.parallel_utils.parallel_state import (
    get_tensor_model_parall",code
ChatTTS/model/velocity/output.py,code,0,"from typing import List, Optional
import torch

from .sequence import (
    PromptLogprobs,
    SampleLogprobs,
    SequenceGroup,
    SequenceStatus,
)


class CompletionOutput:
    """"""The output data of one completion output of a request.

    Args:
        index: The index of the output in the request.
        text: The generated output text.
        token_ids: The token IDs of the generated output text.
        cumulative_logprob: The cumulative log probability of the generated
            output text.
        logprobs: The log probabilities of the top probability words at each
            position if the logprobs are requested.
        finish_reason: The reason why the sequence is finished.
    """"""

    def __init__(
        self,
        index: int,
        text: str,
        token_ids: List[int],
        cumulative_logprob: float,
        logprobs: Optional[SampleLogprobs],
        finish_reason: Optional[str] = None,
        hidden_states: Optional[torch.Tensor] = None,
    ) -> None:
        self.index = index
        self.text = text
        self.token_ids = token_ids
        self.cumulative_logprob = cumulative_logprob
        self.logprobs = logprobs
        self.finish_reason = finish_reason
        self.hidden_states = hidden_states

    def finished(self) -> bool:
        return self.finish_reason is not None

    def __repr__(self) -> str:
        return (
            f""CompletionOutput(index={self.index}, ""
            f""text={self.text!r}, ""
            f""token_ids={self.token_ids}, ""
            f""cumulative_logprob={self.cumulative_logprob}, ""
            f""logprobs={self.logprobs}, ""
            f""finish_reason={self.finish_reason}, ""
            f""hidden_states={self.hidden_states.shape if self.hidden_states is not None else None})""
        )


class RequestOutput:
    """"""The output data of a request to the LLM.

    Args:
        request_id: The unique ID of the request.
        prompt: The prompt string of the request.
        prompt_token_id",code
ChatTTS/model/velocity/sequence.py,code,0,"""""""Sequence and its related classes.""""""

import copy
import enum
from typing import Dict, List, Optional, Union
import torch
from vllm.block import LogicalTokenBlock
from .sampling_params import SamplingParams

PromptLogprobs = List[Optional[Dict[int, float]]]
SampleLogprobs = List[Dict[int, float]]


class SequenceStatus(enum.Enum):
    """"""Status of a sequence.""""""

    WAITING = enum.auto()
    RUNNING = enum.auto()
    SWAPPED = enum.auto()
    FINISHED_STOPPED = enum.auto()
    FINISHED_LENGTH_CAPPED = enum.auto()
    FINISHED_ABORTED = enum.auto()
    FINISHED_IGNORED = enum.auto()

    @staticmethod
    def is_finished(status: ""SequenceStatus"") -> bool:
        return status in [
            SequenceStatus.FINISHED_STOPPED,
            SequenceStatus.FINISHED_LENGTH_CAPPED,
            SequenceStatus.FINISHED_ABORTED,
            SequenceStatus.FINISHED_IGNORED,
        ]

    @staticmethod
    def get_finished_reason(status: ""SequenceStatus"") -> Union[str, None]:
        if status == SequenceStatus.FINISHED_STOPPED:
            finish_reason = ""stop""
        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:
            finish_reason = ""length""
        elif status == SequenceStatus.FINISHED_ABORTED:
            finish_reason = ""abort""
        elif status == SequenceStatus.FINISHED_IGNORED:
            # The ignored sequences are the sequences whose prompt lengths
            # are longer than the model's length cap. Therefore, the stop
            # reason should also be ""length"" as in OpenAI API.
            finish_reason = ""length""
        else:
            finish_reason = None
        return finish_reason


class SequenceData:
    """"""Data associated with a sequence.


    Args:
        prompt_token_ids: The token IDs of the prompt.

    Attributes:
        prompt_token_ids: The token IDs of the prompt.
        output_token_ids: The token IDs of the output.
        cumulative_logprob: The cumulative log probability of the output.
    """"""

    def __init__",code
ChatTTS/model/velocity/worker.py,code,0,"""""""A GPU worker class.""""""

import os
from typing import Dict, List, Optional, Tuple

import torch
import torch.distributed

from vllm.config import CacheConfig, ModelConfig, ParallelConfig, SchedulerConfig
from vllm.model_executor import set_random_seed
from vllm.model_executor.parallel_utils.communication_op import broadcast_object_list
from vllm.model_executor.parallel_utils.parallel_state import initialize_model_parallel
from vllm.sequence import SamplerOutput, SequenceGroupMetadata
from vllm.worker.cache_engine import CacheEngine

from .model_runner import ModelRunner


class Worker:
    """"""A worker class that executes (a partition of) the model on a GPU.

    Each worker is associated with a single GPU. The worker is responsible for
    maintaining the KV cache and executing the model on the GPU. In case of
    distributed inference, each worker is assigned a partition of the model.
    """"""

    def __init__(
        self,
        model_config: ModelConfig,
        parallel_config: ParallelConfig,
        scheduler_config: SchedulerConfig,
        local_rank: int,
        rank: int,
        distributed_init_method: str,
        post_model_path: str,
        is_driver_worker: bool = False,
    ) -> None:
        self.model_config = model_config
        self.parallel_config = parallel_config
        self.scheduler_config = scheduler_config
        self.local_rank = local_rank
        self.rank = rank
        self.distributed_init_method = distributed_init_method
        self.is_driver_worker = is_driver_worker
        self.post_model_path = post_model_path

        if self.is_driver_worker:
            assert self.rank == 0, ""The driver worker must have rank 0.""

        self.model_runner = ModelRunner(
            model_config,
            parallel_config,
            scheduler_config,
            is_driver_worker,
            post_model_path,
        )
        # Uninitialized cache engine. Will be initialized by
        # self.init_cache_engine().
        self.",code
ChatTTS/model/velocity/model_runner.py,code,0,"import time
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn as nn

from .configs import ModelConfig, ParallelConfig, SchedulerConfig
from vllm.logger import init_logger
from .model_loader import get_model
from vllm.model_executor import InputMetadata, SamplingMetadata
from vllm.model_executor.parallel_utils.communication_op import (
    broadcast,
    broadcast_object_list,
)
from .sampling_params import SamplingParams, SamplingType
from .sequence import (
    SamplerOutput,
    SequenceData,
    SequenceGroupMetadata,
    SequenceGroupOutput,
    SequenceOutput,
)
from vllm.utils import in_wsl
from ..embed import Embed
from .sampler import Sampler
from safetensors.torch import safe_open

logger = init_logger(__name__)

KVCache = Tuple[torch.Tensor, torch.Tensor]
_PAD_SLOT_ID = -1
# Capture graphs for batch size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.
# NOTE: _get_graph_batch_size needs to be updated if this list is changed.
_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [8 * i for i in range(1, 33)]


class ModelRunner:

    def __init__(
        self,
        model_config: ModelConfig,
        parallel_config: ParallelConfig,
        scheduler_config: SchedulerConfig,
        is_driver_worker: bool = False,
        post_model_path: str = None,
    ):
        self.model_config = model_config
        self.parallel_config = parallel_config
        self.scheduler_config = scheduler_config
        self.is_driver_worker = is_driver_worker
        self.post_model_path = post_model_path

        # model_config can be None in tests/samplers/test_sampler.py.
        # FIXME(woosuk): This is a hack to make the tests work. Refactor this.
        self.sliding_window = (
            model_config.get_sliding_window() if model_config is not None else None
        )
        self.model = None
        self.block_size = None  # Set after initial profiling.

        self.graph_runners: Dict[int, CUDAGraphRunner] = {}
        self.graph_memor",code
ChatTTS/model/velocity/block_manager.py,code,0,"""""""A block manager that manages token blocks.""""""

import enum
from typing import Dict, List, Optional, Set, Tuple

from vllm.block import PhysicalTokenBlock
from .sequence import Sequence, SequenceGroup, SequenceStatus
from vllm.utils import Device

# Mapping: logical block number -> physical block.
BlockTable = List[PhysicalTokenBlock]


class BlockAllocator:
    """"""Manages free physical token blocks for a device.

    The allocator maintains a list of free blocks and allocates a block when
    requested. When a block is freed, its reference count is decremented. If
    the reference count becomes zero, the block is added back to the free list.
    """"""

    def __init__(
        self,
        device: Device,
        block_size: int,
        num_blocks: int,
    ) -> None:
        self.device = device
        self.block_size = block_size
        self.num_blocks = num_blocks

        # Initialize the free blocks.
        self.free_blocks: BlockTable = []
        for i in range(num_blocks):
            block = PhysicalTokenBlock(
                device=device, block_number=i, block_size=block_size
            )
            self.free_blocks.append(block)

    def allocate(self) -> PhysicalTokenBlock:
        if not self.free_blocks:
            raise ValueError(""Out of memory! No free blocks are available."")
        block = self.free_blocks.pop()
        block.ref_count = 1
        return block

    def free(self, block: PhysicalTokenBlock) -> None:
        if block.ref_count == 0:
            raise ValueError(f""Double free! {block} is already freed."")
        block.ref_count -= 1
        if block.ref_count == 0:
            self.free_blocks.append(block)

    def get_num_free_blocks(self) -> int:
        return len(self.free_blocks)


class AllocStatus(enum.Enum):
    """"""Result for BlockSpaceManager.can_allocate

    1. Ok: seq_group can be allocated now.
    2. Later: seq_group cannot be allocated.
      The capacity of allocator is larger than seq_group required.
  ",code
ChatTTS/model/velocity/model_loader.py,code,0,"""""""Utilities for selecting and loading models.""""""

import contextlib

import torch
import torch.nn as nn

from vllm.config import ModelConfig
from vllm.model_executor.models import ModelRegistry
from vllm.model_executor.weight_utils import get_quant_config, initialize_dummy_weights

from .llama import LlamaModel


@contextlib.contextmanager
def _set_default_torch_dtype(dtype: torch.dtype):
    """"""Sets the default torch dtype to the given dtype.""""""
    old_dtype = torch.get_default_dtype()
    torch.set_default_dtype(dtype)
    yield
    torch.set_default_dtype(old_dtype)


def get_model(model_config: ModelConfig) -> nn.Module:
    # Get the (maybe quantized) linear method.
    linear_method = None
    if model_config.quantization is not None:
        quant_config = get_quant_config(
            model_config.quantization,
            model_config.model,
            model_config.hf_config,
            model_config.download_dir,
        )
        capability = torch.cuda.get_device_capability()
        capability = capability[0] * 10 + capability[1]
        if capability < quant_config.get_min_capability():
            raise ValueError(
                f""The quantization method {model_config.quantization} is not ""
                ""supported for the current GPU. ""
                f""Minimum capability: {quant_config.get_min_capability()}. ""
                f""Current capability: {capability}.""
            )
        supported_dtypes = quant_config.get_supported_act_dtypes()
        if model_config.dtype not in supported_dtypes:
            raise ValueError(
                f""{model_config.dtype} is not supported for quantization ""
                f""method {model_config.quantization}. Supported dtypes: ""
                f""{supported_dtypes}""
            )
        linear_method = quant_config.get_linear_method()

    with _set_default_torch_dtype(model_config.dtype):
        # Create a model instance.
        # The weights will be initialized as empty tensors.
        with to",code
ChatTTS/model/velocity/scheduler.py,code,0,"import enum
import time
from typing import Dict, Iterable, List, Optional, Tuple, Union

from vllm.config import CacheConfig, SchedulerConfig
from .block_manager import AllocStatus, BlockSpaceManager
from vllm.core.policy import PolicyFactory
from vllm.logger import init_logger
from .sequence import (
    Sequence,
    SequenceData,
    SequenceGroup,
    SequenceGroupMetadata,
    SequenceStatus,
)

logger = init_logger(__name__)


class PreemptionMode(enum.Enum):
    """"""Preemption modes.

    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory
    and swap them back in when the sequences are resumed.
    2. Recomputation: Discard the blocks of the preempted sequences and
    recompute them when the sequences are resumed, treating the sequences as
    new prompts.
    """"""

    SWAP = enum.auto()
    RECOMPUTE = enum.auto()


class SchedulerOutputs:

    def __init__(
        self,
        scheduled_seq_groups: List[SequenceGroup],
        prompt_run: bool,
        num_batched_tokens: int,
        blocks_to_swap_in: Dict[int, int],
        blocks_to_swap_out: Dict[int, int],
        blocks_to_copy: Dict[int, List[int]],
        ignored_seq_groups: List[SequenceGroup],
    ) -> None:
        self.scheduled_seq_groups = scheduled_seq_groups
        self.prompt_run = prompt_run
        self.num_batched_tokens = num_batched_tokens
        self.blocks_to_swap_in = blocks_to_swap_in
        self.blocks_to_swap_out = blocks_to_swap_out
        self.blocks_to_copy = blocks_to_copy
        # Swap in and swap out should never happen at the same time.
        assert not (blocks_to_swap_in and blocks_to_swap_out)
        self.ignored_seq_groups = ignored_seq_groups

    def is_empty(self) -> bool:
        # NOTE: We do not consider the ignored sequence groups.
        return (
            not self.scheduled_seq_groups
            and not self.blocks_to_swap_in
            and not self.blocks_to_swap_out
            and not self.blocks_to_copy
        )


c",code
ChatTTS/model/velocity/llm.py,code,0,"from typing import List, Optional, Union

from tqdm import tqdm
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
from vllm.utils import Counter

from .configs import EngineArgs
from .llm_engine import LLMEngine
from .output import RequestOutput
from .sampling_params import SamplingParams


class LLM:
    """"""An LLM for generating texts from given prompts and sampling parameters.

    This class includes a tokenizer, a language model (possibly distributed
    across multiple GPUs), and GPU memory space allocated for intermediate
    states (aka KV cache). Given a batch of prompts and sampling parameters,
    this class generates texts from the model, using an intelligent batching
    mechanism and efficient memory management.

    NOTE: This class is intended to be used for offline inference. For online
    serving, use the `AsyncLLMEngine` class instead.
    NOTE: For the comprehensive list of arguments, see `EngineArgs`.

    Args:
        model: The name or path of a HuggingFace Transformers model.
        tokenizer: The name or path of a HuggingFace Transformers tokenizer.
        tokenizer_mode: The tokenizer mode. ""auto"" will use the fast tokenizer
            if available, and ""slow"" will always use the slow tokenizer.
        trust_remote_code: Trust remote code (e.g., from HuggingFace) when
            downloading the model and tokenizer.
        tensor_parallel_size: The number of GPUs to use for distributed
            execution with tensor parallelism.
        dtype: The data type for the model weights and activations. Currently,
            we support `float32`, `float16`, and `bfloat16`. If `auto`, we use
            the `torch_dtype` attribute specified in the model config file.
            However, if the `torch_dtype` in the config is `float32`, we will
            use `float16` instead.
        quantization: The method used to quantize the model weights. Currently,
            we support ""awq"", ""gptq"" and ""squeezellm"". If None, w",code
ChatTTS/model/velocity/llm_engine.py,code,0,"import copy
from collections import defaultdict
import os
import time
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Tuple, Union

from vllm.config import CacheConfig, ModelConfig, ParallelConfig, SchedulerConfig
from .scheduler import Scheduler, SchedulerOutputs
from .configs import EngineArgs
from vllm.engine.metrics import record_metrics
from vllm.engine.ray_utils import RayWorkerVllm, initialize_cluster, ray
from vllm.logger import init_logger
from .output import RequestOutput
from .sampling_params import SamplingParams
from .sequence import (
    SamplerOutput,
    Sequence,
    SequenceGroup,
    SequenceGroupOutput,
    SequenceOutput,
    SequenceStatus,
)
from vllm.transformers_utils.tokenizer import detokenize_incrementally, get_tokenizer
from vllm.utils import Counter, set_cuda_visible_devices, get_ip, get_open_port
import numpy as np

if ray:
    from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

if TYPE_CHECKING:
    from ray.util.placement_group import PlacementGroup

logger = init_logger(__name__)

_LOGGING_INTERVAL_SEC = 5


class LLMEngine:
    """"""An LLM engine that receives requests and generates texts.

    This is the main class for the vLLM engine. It receives requests
    from clients and generates texts from the LLM. It includes a tokenizer, a
    language model (possibly distributed across multiple GPUs), and GPU memory
    space allocated for intermediate states (aka KV cache). This class utilizes
    iteration-level scheduling and efficient memory management to maximize the
    serving throughput.

    The `LLM` class wraps this class for offline batched inference and the
    `AsyncLLMEngine` class wraps this class for online serving.

    NOTE: The config arguments are derived from the `EngineArgs` class. For the
    comprehensive list of arguments, see `EngineArgs`.

    Args:
        model_config: The configuration related to the LLM model.
        cache_config: The configuration related ",code
ChatTTS/model/velocity/sampling_params.py,code,0,"""""""Sampling parameters for text generation.""""""

from enum import IntEnum
from functools import cached_property
from typing import Callable, List, Optional, Union

import torch

_SAMPLING_EPS = 1e-5


class SamplingType(IntEnum):
    GREEDY = 0
    RANDOM = 1
    BEAM = 2


LogitsProcessor = Callable[[List[int], torch.Tensor], torch.Tensor]
""""""LogitsProcessor is a function that takes a list of previously generated
tokens and a tensor of the logits for the next token, and returns a modified
tensor of logits to sample from.""""""


class SamplingParams:
    """"""Sampling parameters for text generation.

    Overall, we follow the sampling parameters from the OpenAI text completion
    API (https://platform.openai.com/docs/api-reference/completions/create).
    In addition, we support beam search, which is not supported by OpenAI.

    Args:
        n: Number of output sequences to return for the given prompt.
        best_of: Number of output sequences that are generated from the prompt.
            From these `best_of` sequences, the top `n` sequences are returned.
            `best_of` must be greater than or equal to `n`. This is treated as
            the beam width when `use_beam_search` is True. By default, `best_of`
            is set to `n`.
        presence_penalty: Float that penalizes new tokens based on whether they
            appear in the generated text so far. Values > 0 encourage the model
            to use new tokens, while values < 0 encourage the model to repeat
            tokens.
        frequency_penalty: Float that penalizes new tokens based on their
            frequency in the generated text so far. Values > 0 encourage the
            model to use new tokens, while values < 0 encourage the model to
            repeat tokens.
        repetition_penalty: Float that penalizes new tokens based on whether
            they appear in the prompt and the generated text so far. Values > 1
            encourage the model to use new tokens, while values < 1 ",code
ChatTTS/model/velocity/sampler.py,code,0,"import torch
from torch.functional import F
from typing import List, Callable

from ..embed import Embed


class Sampler:
    def __init__(self, post_model: Embed, num_audio_tokens: int, num_vq: int):
        self.post_model = post_model
        self.device = next(self.post_model.parameters()).device
        self.num_audio_tokens = num_audio_tokens
        self.num_vq = num_vq

    def sample(
        self,
        inputs_ids: torch.Tensor,
        hidden_states: torch.Tensor,
        infer_text: bool = False,
        temperature: torch.Tensor = 1.0,
        logits_processors: List[Callable] = [
            lambda logits_token, logits: logits,
        ],
        logits_warpers: List[Callable] = [
            lambda logits_token, logits: logits,
        ],
        min_new_token: int = 0,
        now_length: int = 0,
        eos_token: int = 0,
        start_idx: int = 0,
    ):
        # print(inputs_ids.shape)
        B = hidden_states.shape[0]

        end_idx = torch.zeros(
            inputs_ids.shape[0], device=inputs_ids.device, dtype=torch.long
        )
        finish = torch.zeros(inputs_ids.shape[0], device=inputs_ids.device).bool()
        if not infer_text:
            temperature = (
                temperature.unsqueeze(0)
                .expand(inputs_ids.shape[0], -1)
                .contiguous()
                .view(-1, 1)
            )

        if infer_text:
            logits: torch.Tensor = self.post_model.head_text(hidden_states)
        else:
            # logits = torch.stack([self.head_code[i](hidden_states) for i in range(self.num_vq)], 3)
            logits = torch.empty(
                hidden_states.size(0),
                hidden_states.size(1),
                self.num_audio_tokens,
                self.num_vq,
                dtype=torch.float,
                device=self.device,
            )
            for num_vq_iter in range(self.num_vq):
                x: torch.Tensor = self.post_model.head_code[num_vq_iter](hidden_states)
  ",code
ChatTTS/model/velocity/__init__.py,code,0,"from .llm import LLM
from .sampling_params import SamplingParams
",code
ChatTTS/model/velocity/configs.py,code,0,"from typing import Optional, Union, Tuple
import os

import torch
from transformers import PretrainedConfig

from vllm.logger import init_logger
from vllm.transformers_utils.config import get_config
from vllm.utils import get_cpu_memory, is_hip

import argparse
import dataclasses
from dataclasses import dataclass


logger = init_logger(__name__)

_GB = 1 << 30


class ModelConfig:
    """"""Configuration for the model.

    Args:
        model: Name or path of the huggingface model to use.
        tokenizer: Name or path of the huggingface tokenizer to use.
        tokenizer_mode: Tokenizer mode. ""auto"" will use the fast tokenizer if
            available, and ""slow"" will always use the slow tokenizer.
        trust_remote_code: Trust remote code (e.g., from HuggingFace) when
            downloading the model and tokenizer.
        download_dir: Directory to download and load the weights, default to the
            default cache directory of huggingface.
        load_format: The format of the model weights to load:
            ""auto"" will try to load the weights in the safetensors format and
                fall back to the pytorch bin format if safetensors format is
                not available.
            ""pt"" will load the weights in the pytorch bin format.
            ""safetensors"" will load the weights in the safetensors format.
            ""npcache"" will load the weights in pytorch format and store
                a numpy cache to speed up the loading.
            ""dummy"" will initialize the weights with random values, which is
                mainly for profiling.
        dtype: Data type for model weights and activations. The ""auto"" option
            will use FP16 precision for FP32 and FP16 models, and BF16 precision
            for BF16 models.
        seed: Random seed for reproducibility.
        revision: The specific model version to use. It can be a branch name,
            a tag name, or a commit id. If unspecified, will use the default
            v",code
ChatTTS/model/cuda/te_llama.py,code,0,"# Copyright (c) 2022-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# See LICENSE for license information.
#
# From https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/te_llama/te_llama.py
#
# Edited by fumiama.

import re
from contextlib import contextmanager
from typing import Dict

import transformer_engine as te
from transformer_engine.pytorch.attention import RotaryPositionEmbedding

import torch

import transformers
from transformers.models.llama.modeling_llama import (
    LlamaModel,
    LlamaConfig,
)
from transformers.modeling_utils import _load_state_dict_into_model

from .patch import LlamaRMSNorm


@contextmanager
def replace_decoder(te_decoder_cls, llama_rms_norm_cls):
    """"""
    Replace `LlamaDecoderLayer` with custom `TELlamaDecoderLayer`.
    """"""
    original_llama_decoder_cls = (
        transformers.models.llama.modeling_llama.LlamaDecoderLayer
    )
    transformers.models.llama.modeling_llama.LlamaDecoderLayer = te_decoder_cls
    original_llama_rms_norm_cls = transformers.models.llama.modeling_llama.LlamaRMSNorm
    transformers.models.llama.modeling_llama.LlamaRMSNorm = llama_rms_norm_cls
    try:
        yield
    finally:
        transformers.models.llama.modeling_llama.LlamaDecoderLayer = (
            original_llama_decoder_cls
        )
        transformers.models.llama.modeling_llama.LlamaRMSNorm = (
            original_llama_rms_norm_cls
        )


class TELlamaDecoderLayer(te.pytorch.TransformerLayer):
    """"""
    Wrapper class over TE's `TransformerLayer`. This makes the wrapper very
    similar to HF's `LlamaDecoderLayer` and easier to replace it in the code.

    Args:
        config: LlamaConfig
        args: positional args (for compatibility with `LlamaDecoderLayer`)
        kwargs: keyword args (for compatibility with `LlamaDecoderLayer`)
    """"""

    def __init__(self, config, *args, **kwargs):
        super().__init__(
            hidden_size=config.hidden_size,
            ffn_hidden_size=con",code
ChatTTS/model/cuda/patch.py,code,0,"import torch


class LlamaRMSNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """"""
        LlamaRMSNorm is equivalent to T5LayerNorm
        """"""
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight.to(hidden_states.device) * hidden_states.to(input_dtype)
",code
ChatTTS/model/cuda/__init__.py,code,0,"from .te_llama import TELlamaModel
",code
examples/onnx/gpt.py,code,0,"import logging
from typing import Tuple

import torch
import torch.nn as nn
from torch.nn.utils.parametrizations import weight_norm

from modeling_llama import LlamaModel, LlamaConfig


class GPT(nn.Module):
    def __init__(
        self,
        gpt_config: dict,
        num_audio_tokens: int = 626,
        num_text_tokens: int = 21178,
        num_vq=4,
        use_flash_attn=False,
        device=torch.device(""cpu""),
        logger=logging.getLogger(__name__),
    ):
        super().__init__()

        self.logger = logger

        self.device = device
        self.device_gpt = device if ""mps"" not in str(device) else torch.device(""cpu"")

        self.num_vq = num_vq
        self.num_audio_tokens = num_audio_tokens

        self.use_flash_attn = use_flash_attn

        self.gpt, self.llama_config = self._build_llama(gpt_config, self.device_gpt)
        self.is_te_llama = False
        self.model_dim = int(self.gpt.config.hidden_size)
        self.emb_code = nn.ModuleList(
            [
                nn.Embedding(
                    num_audio_tokens,
                    self.model_dim,
                    device=self.device_gpt,
                )
                for _ in range(num_vq)
            ],
        )
        self.emb_text = nn.Embedding(
            num_text_tokens, self.model_dim, device=self.device_gpt
        )

        self.head_text = weight_norm(
            nn.Linear(
                self.model_dim,
                num_text_tokens,
                bias=False,
                device=device,
            ),
            name=""weight"",
        )
        self.head_code = nn.ModuleList(
            [
                weight_norm(
                    nn.Linear(
                        self.model_dim,
                        num_audio_tokens,
                        bias=False,
                        device=device,
                    ),
                    name=""weight"",
                )
                for _ in range(self.num_vq)
            ],
      ",code
examples/onnx/modeling_llama.py,code,0,"# coding=utf-8
# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""
PyTorch LLaMA model.
Copied from https://github.com/sophgo/LLM-TPU/blob/main/models/Llama2/compile/files/llama-2-7b-chat-hf/modeling_llama.py
""""""
import math
from typing import List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch import nn
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss

from transformers.activations import ACT2FN
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    SequenceClassifierOutputWithPast,
)
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import (
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    logging,
    replace_return_docstrings,
)
from transformers.models.llama.configuration_llama import LlamaConfig


logger = logging.get_logger(__name__)

_CONFIG_FOR_DOC = ""LlamaConfig""


# Copied from transformers.models.bart.modeling_bart._make_causal_mask
def _make_causal_mask(
    input_ids_shape: torch.Size,
    dtype: torch.dtype,
    device",code
examples/onnx/exporter.py,code,0,"import os, sys

if sys.platform == ""darwin"":
    os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""

now_dir = os.getcwd()
sys.path.append(now_dir)

from dataclasses import asdict
import argparse
import torch
from tqdm import tqdm
from ChatTTS.model.dvae import DVAE
from ChatTTS.config import Config
from vocos import Vocos
from vocos.pretrained import instantiate_class
import torch.jit as jit

from gpt import GPT

# disable cuda
torch.cuda.is_available = lambda: False

# add args to control which modules to export
parser = argparse.ArgumentParser()
parser.add_argument(""--gpt"", action=""store_true"", help=""trace gpt"")
parser.add_argument(""--decoder"", action=""store_true"", help=""trace decoder"")
parser.add_argument(""--vocos"", action=""store_true"", help=""trace vocos"")
parser.add_argument(
    ""--pth_dir"", default=""./assets"", type=str, help=""path to the pth model directory""
)
parser.add_argument(
    ""--out_dir"", default=""./tmp"", type=str, help=""path to output directory""
)

args = parser.parse_args()
chattts_config = Config()


def export_gpt():
    gpt_model = GPT(gpt_config=asdict(chattts_config.gpt), use_flash_attn=False).eval()
    gpt_model.from_pretrained(asdict(chattts_config.path)[""gpt_ckpt_path""])
    gpt_model = gpt_model.eval()
    for param in gpt_model.parameters():
        param.requires_grad = False

    config = gpt_model.gpt.config
    layers = gpt_model.gpt.layers
    model_norm = gpt_model.gpt.norm

    NUM_OF_LAYERS = config.num_hidden_layers
    HIDDEN_SIZE = config.hidden_size
    NUM_ATTENTION_HEADS = config.num_attention_heads
    NUM_KEY_VALUE_HEADS = config.num_key_value_heads
    HEAD_DIM = HIDDEN_SIZE // NUM_ATTENTION_HEADS  # 64
    TEXT_VOCAB_SIZE = gpt_model.emb_text.weight.shape[0]
    AUDIO_VOCAB_SIZE = gpt_model.emb_code[0].weight.shape[0]
    SEQ_LENGTH = 512

    folder = os.path.join(args.out_dir, ""gpt"")
    os.makedirs(folder, exist_ok=True)

    for param in gpt_model.emb_text.parameters():
        param.requires_grad = False

    for para",code
examples/cmd/run.py,code,0,"import os, sys

if sys.platform == ""darwin"":
    os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""

now_dir = os.getcwd()
sys.path.append(now_dir)

from typing import Optional, List
import argparse

import numpy as np

import ChatTTS

from tools.logger import get_logger
from tools.audio import pcm_arr_to_mp3_view
from tools.normalizer.en import normalizer_en_nemo_text
from tools.normalizer.zh import normalizer_zh_tn

logger = get_logger(""Command"")


def save_mp3_file(wav, index):
    data = pcm_arr_to_mp3_view(wav)
    mp3_filename = f""output_audio_{index}.mp3""
    with open(mp3_filename, ""wb"") as f:
        f.write(data)
    logger.info(f""Audio saved to {mp3_filename}"")


def load_normalizer(chat: ChatTTS.Chat):
    # try to load normalizer
    try:
        chat.normalizer.register(""en"", normalizer_en_nemo_text())
    except ValueError as e:
        logger.error(e)
    except BaseException:
        logger.warning(""Package nemo_text_processing not found!"")
        logger.warning(
            ""Run: conda install -c conda-forge pynini=2.1.5 && pip install nemo_text_processing"",
        )
    try:
        chat.normalizer.register(""zh"", normalizer_zh_tn())
    except ValueError as e:
        logger.error(e)
    except BaseException:
        logger.warning(""Package WeTextProcessing not found!"")
        logger.warning(
            ""Run: conda install -c conda-forge pynini=2.1.5 && pip install WeTextProcessing"",
        )


def main(
    texts: List[str],
    spk: Optional[str] = None,
    stream: bool = False,
    source: str = ""local"",
    custom_path: str = """",
):
    logger.info(""Text input: %s"", str(texts))

    chat = ChatTTS.Chat(get_logger(""ChatTTS""))
    logger.info(""Initializing ChatTTS..."")
    load_normalizer(chat)

    is_load = False
    if os.path.isdir(custom_path) and source == ""custom"":
        is_load = chat.load(source=""custom"", custom_path=custom_path)
    else:
        is_load = chat.load(source=source)

    if is_load:
        logger.info(""Models loade",code
examples/cmd/stream.py,code,0,"import random

import numpy as np

from tools.audio import float_to_int16


# 
class ChatStreamer:
    def __init__(self, base_block_size=8000):
        self.base_block_size = base_block_size

    # streamstream
    @staticmethod
    def _update_stream(history_stream_wav, new_stream_wav, thre):
        if history_stream_wav is not None:
            result_stream = np.concatenate([history_stream_wav, new_stream_wav], axis=1)
            is_keep_next = result_stream.shape[0] * result_stream.shape[1] < thre
            if random.random() > 0.1:
                print(
                    ""update_stream"",
                    is_keep_next,
                    [i.shape if i is not None else None for i in result_stream],
                )
        else:
            result_stream = new_stream_wav
            is_keep_next = result_stream.shape[0] * result_stream.shape[1] < thre

        return result_stream, is_keep_next

    # batch
    @staticmethod
    def _accum(accum_wavs, stream_wav):
        if accum_wavs is None:
            accum_wavs = stream_wav
        else:
            accum_wavs = np.concatenate([accum_wavs, stream_wav], axis=1)
        return accum_wavs

    # batch stream
    @staticmethod
    def batch_stream_formatted(stream_wav, output_format=""PCM16_byte""):
        if output_format in (""PCM16_byte"", ""PCM16""):
            format_data = float_to_int16(stream_wav)
        else:
            format_data = stream_wav
        return format_data

    # 
    @staticmethod
    def formatted(data, output_format=""PCM16_byte""):
        if output_format == ""PCM16_byte"":
            format_data = data.astype(""<i2"").tobytes()
        else:
            format_data = data
        return format_data

    # 
    @staticmethod
    def checkvoice(data):
        if np.abs(data).max() < 1e-6:
            return False
        else:
            return True

    # 
    @staticmethod
    de",code
examples/api/openai_api.py,code,0,"""""""
openai_api.py
This module implements a FastAPI-based text-to-speech API compatible with OpenAI's interface specification.

Main features and improvements:
- Use app.state to manage global state, ensuring thread safety
- Add exception handling and unified error responses to improve stability
- Support multiple voice options and audio formats for greater flexibility
- Add input validation to ensure the validity of request parameters
- Support additional OpenAI TTS parameters (e.g., speed) for richer functionality
- Implement health check endpoint for easy service status monitoring
- Use asyncio.Lock to manage model access, improving concurrency performance
- Load and manage speaker embedding files to support personalized speech synthesis
""""""

import io
import os
import sys
import asyncio
import time
from typing import Optional, Dict
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
import torch

# Cross-platform compatibility settings
if sys.platform == ""darwin"":
    os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""

# Set working directory and add to system path
now_dir = os.getcwd()
sys.path.append(now_dir)

# Import necessary modules
import ChatTTS
from tools.audio import pcm_arr_to_mp3_view, pcm_arr_to_ogg_view, pcm_arr_to_wav_view
from tools.logger import get_logger
from tools.normalizer.en import normalizer_en_nemo_text
from tools.normalizer.zh import normalizer_zh_tn

# Initialize logger
logger = get_logger(""Command"")

# Initialize FastAPI application
app = FastAPI()

# Voice mapping table
# Download stable voices:
# ModelScope Community: https://modelscope.cn/studios/ttwwwaa/ChatTTS_Speaker
# HuggingFace: https://huggingface.co/spaces/taa/ChatTTS_Speaker
VOICE_MAP = {
    ""default"": ""1528.pt"",
    ""alloy"": ""1384.pt"",
    ""echo"": ""2443.pt"",
}

# Allowed audio formats
ALLOWED_FORMATS = {""mp3"", ""wav"", ""ogg""}


@app.on_event(""startup"")
async def startup_event():
    """"""L",code
examples/api/main.py,code,0,"import io
import os
import sys
import zipfile

from fastapi import FastAPI
from fastapi.responses import StreamingResponse


if sys.platform == ""darwin"":
    os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""

now_dir = os.getcwd()
sys.path.append(now_dir)

from typing import Optional

import ChatTTS

from tools.audio import pcm_arr_to_mp3_view
from tools.logger import get_logger
import torch


from pydantic import BaseModel
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from tools.normalizer.en import normalizer_en_nemo_text
from tools.normalizer.zh import normalizer_zh_tn

logger = get_logger(""Command"")

app = FastAPI()


@app.on_event(""startup"")
async def startup_event():
    global chat

    chat = ChatTTS.Chat(get_logger(""ChatTTS""))
    chat.normalizer.register(""en"", normalizer_en_nemo_text())
    chat.normalizer.register(""zh"", normalizer_zh_tn())

    logger.info(""Initializing ChatTTS..."")
    if chat.load(source=""huggingface""):
        logger.info(""Models loaded successfully."")
    else:
        logger.error(""Models load failed."")
        sys.exit(1)


@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc: RequestValidationError):
    logger.error(f""Validation error: {exc.errors()}"")
    return JSONResponse(status_code=422, content={""detail"": exc.errors()})


class ChatTTSParams(BaseModel):
    text: list[str]
    stream: bool = False
    lang: Optional[str] = None
    skip_refine_text: bool = False
    refine_text_only: bool = False
    use_decoder: bool = True
    do_text_normalization: bool = True
    do_homophone_replacement: bool = False
    params_refine_text: ChatTTS.Chat.RefineTextParams = None
    params_infer_code: ChatTTS.Chat.InferCodeParams


@app.post(""/generate_voice"")
async def generate_voice(params: ChatTTSParams):
    logger.info(""Text input: %s"", str(params.text))

    # audio seed
    if params.params_infer_code.manual_seed is not None:
        torc",code
examples/api/postScript.py,code,0,"import argparse
import datetime
import os
import zipfile
from io import BytesIO

import requests

chattts_service_host = os.environ.get(""CHATTTS_SERVICE_HOST"", ""127.0.0.1"")
chattts_service_port = os.environ.get(""CHATTTS_SERVICE_PORT"", ""9900"")

CHATTTS_URL = f""http://{chattts_service_host}:{chattts_service_port}/generate_voice""


def parse_arguments():
    parser = argparse.ArgumentParser(description=""HTTP client for ChatTTS service"")
    parser.add_argument(
        ""--text"", type=str, nargs=""+"", required=True, help=""Text to synthesize""
    )
    parser.add_argument(
        ""--audio_seed"", type=int, required=True, help=""Audio generation seed""
    )
    parser.add_argument(
        ""--text_seed"", type=int, required=True, help=""Text generation seed""
    )
    parser.add_argument(
        ""--stream"", type=bool, default=False, help=""Enable/disable streaming""
    )
    parser.add_argument(""--lang"", type=str, default=None, help=""Language code for text"")
    parser.add_argument(
        ""--skip_refine_text"", type=bool, default=True, help=""Skip text refinement""
    )
    parser.add_argument(
        ""--refine_text_only"", type=bool, default=False, help=""Only refine text""
    )
    parser.add_argument(
        ""--use_decoder"", type=bool, default=True, help=""Use decoder during inference""
    )
    parser.add_argument(
        ""--do_text_normalization"",
        type=bool,
        default=True,
        help=""Enable text normalization"",
    )
    parser.add_argument(
        ""--do_homophone_replacement"",
        type=bool,
        default=False,
        help=""Enable homophone replacement"",
    )
    parser.add_argument(
        ""--tgt"",
        type=str,
        default=""./output"",
        help=""Target directory to save output files"",
    )
    parser.add_argument(
        ""--filename"",
        type=str,
        default=""test.mp3"",
        help=""Target directory to save output files"",
    )

    # Refinement text parameters
    parser.add_argument(
        ""--refine_prompt"", typ",code
examples/api/client.py,code,0,"import datetime
import os
import zipfile
from io import BytesIO

import requests

chattts_service_host = os.environ.get(""CHATTTS_SERVICE_HOST"", ""localhost"")
chattts_service_port = os.environ.get(""CHATTTS_SERVICE_PORT"", ""8000"")

CHATTTS_URL = f""http://{chattts_service_host}:{chattts_service_port}/generate_voice""


# main infer params
body = {
    ""text"": [
        """",
        """",
    ],
    ""stream"": False,
    ""lang"": None,
    ""skip_refine_text"": True,
    ""refine_text_only"": False,
    ""use_decoder"": True,
    ""audio_seed"": 12345678,
    ""text_seed"": 87654321,
    ""do_text_normalization"": True,
    ""do_homophone_replacement"": False,
}

# refine text params
params_refine_text = {
    ""prompt"": """",
    ""top_P"": 0.7,
    ""top_K"": 20,
    ""temperature"": 0.7,
    ""repetition_penalty"": 1,
    ""max_new_token"": 384,
    ""min_new_token"": 0,
    ""show_tqdm"": True,
    ""ensure_non_empty"": True,
    ""stream_batch"": 24,
}
body[""params_refine_text""] = params_refine_text

# infer code params
params_infer_code = {
    ""prompt"": ""[speed_5]"",
    ""top_P"": 0.1,
    ""top_K"": 20,
    ""temperature"": 0.3,
    ""repetition_penalty"": 1.05,
    ""max_new_token"": 2048,
    ""min_new_token"": 0,
    ""show_tqdm"": True,
    ""ensure_non_empty"": True,
    ""stream_batch"": True,
    ""spk_emb"": None,
}
body[""params_infer_code""] = params_infer_code


try:
    response = requests.post(CHATTTS_URL, json=body)
    response.raise_for_status()
    with zipfile.ZipFile(BytesIO(response.content), ""r"") as zip_ref:
        # save files for each request in a different folder
        dt = datetime.datetime.now()
        ts = int(dt.timestamp())
        tgt = f""./output/{ts}/""
        os.makedirs(tgt, 0o755)
        zip_ref.extractall(tgt)
        print(""Extracted files into"", tgt)

except requests.exceptions.RequestException as e:
    print(f""Request Error: {e}"")
",code
examples/web/ex.py,code,0,"ex = [
    [
        """",
        0.3,
        0.7,
        20,
        2,
        42,
        True,
    ],
    [
        ""What is your favorite english food?"",
        0.5,
        0.5,
        10,
        245,
        531,
        True,
    ],
    [
        ""chat T T S is a text to speech model designed for dialogue applications. [uv_break]it supports mixed language input [uv_break]and offers multi speaker capabilities with precise control over prosodic elements like [uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. [uv_break]it delivers natural and expressive speech,[uv_break]so please[uv_break] use the project responsibly at your own risk.[uv_break]"",
        0.8,
        0.4,
        7,
        70,
        165,
        False,
    ],
]
",code
examples/web/funcs.py,code,0,"import random
from typing import Optional
from time import sleep

import gradio as gr

import sys

sys.path.append("".."")
sys.path.append(""../.."")
from tools.audio import float_to_int16, has_ffmpeg_installed, load_audio
from tools.logger import get_logger

logger = get_logger("" WebUI "")

from tools.seeder import TorchSeedContext
from tools.normalizer import normalizer_en_nemo_text, normalizer_zh_tn

import ChatTTS

chat = ChatTTS.Chat(get_logger(""ChatTTS""))

custom_path: Optional[str] = None

has_interrupted = False
is_in_generate = False

seed_min = 1
seed_max = 4294967295

use_mp3 = has_ffmpeg_installed()
if not use_mp3:
    logger.warning(""no ffmpeg installed, use wav file output"")

# 
voices = {
    ""Default"": {""seed"": 2},
    ""Timbre1"": {""seed"": 1111},
    ""Timbre2"": {""seed"": 2222},
    ""Timbre3"": {""seed"": 3333},
    ""Timbre4"": {""seed"": 4444},
    ""Timbre5"": {""seed"": 5555},
    ""Timbre6"": {""seed"": 6666},
    ""Timbre7"": {""seed"": 7777},
    ""Timbre8"": {""seed"": 8888},
    ""Timbre9"": {""seed"": 9999},
}


def generate_seed():
    return gr.update(value=random.randint(seed_min, seed_max))


# seed
def on_voice_change(vocie_selection):
    return voices.get(vocie_selection)[""seed""]


def on_audio_seed_change(audio_seed_input):
    with TorchSeedContext(audio_seed_input):
        rand_spk = chat.sample_random_speaker()
    return rand_spk


def load_chat(cust_path: Optional[str], coef: Optional[str]) -> bool:
    if cust_path == None:
        ret = chat.load(coef=coef)
    else:
        logger.info(""local model path: %s"", cust_path)
        ret = chat.load(""custom"", custom_path=cust_path, coef=coef)
        global custom_path
        custom_path = cust_path
    if ret:
        try:
            chat.normalizer.register(""en"", normalizer_en_nemo_text())
        except ValueError as e:
            logger.error(e)
        except:
            logger.warning(""Package nemo_text_processing not found!"")
            logger.warning(
                ""Run: conda",code
examples/web/webui.py,code,0,"import os, sys

if sys.platform == ""darwin"":
    os.environ[""PYTORCH_ENABLE_MPS_FALLBACK""] = ""1""

now_dir = os.getcwd()
sys.path.append(now_dir)

import argparse

import gradio as gr

from funcs import *
from ex import ex


def main():

    with gr.Blocks() as demo:
        gr.Markdown(""# ChatTTS WebUI"")
        gr.Markdown(""- **GitHub Repo**: https://github.com/2noise/ChatTTS"")
        gr.Markdown(""- **HuggingFace Repo**: https://huggingface.co/2Noise/ChatTTS"")

        with gr.Row():
            with gr.Column(scale=2):
                text_input = gr.Textbox(
                    label=""Input Text"",
                    lines=4,
                    max_lines=4,
                    placeholder=""Please Input Text..."",
                    value=ex[0][0],
                    interactive=True,
                )
                sample_text_input = gr.Textbox(
                    label=""Sample Text"",
                    lines=4,
                    max_lines=4,
                    placeholder=""If Sample Audio and Sample Text are available, the Speaker Embedding will be disabled."",
                    interactive=True,
                )
            with gr.Column():
                with gr.Tab(label=""Sample Audio""):
                    sample_audio_input = gr.Audio(
                        value=None,
                        type=""filepath"",
                        interactive=True,
                        show_label=False,
                        waveform_options=gr.WaveformOptions(
                            sample_rate=24000,
                        ),
                        scale=1,
                    )
                with gr.Tab(label=""Sample Audio Code""):
                    sample_audio_code_input = gr.Textbox(
                        lines=12,
                        max_lines=12,
                        show_label=False,
                        placeholder=""Paste the Code copied before after uploading Sample Audio."",
                        interactive=True,
 ",code
examples/web/__init__.py,code,0,,code
